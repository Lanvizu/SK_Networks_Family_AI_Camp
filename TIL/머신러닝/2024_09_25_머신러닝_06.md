# 주성분 분석 (Principal Component Analysis, PCA)

주성분 분석은 고차원 데이터를 저차원으로 축소하는 대표적인 기법

데이터의 분산을 최대한 보존하면서 차원을 줄이는 방법으로, 데이터 압축과 시각화에 널리 사용

## 차원과 차원 축소

### 차원의 정의
- 머신러닝에서 **차원**은 데이터가 가진 속성 또는 특성의 수를 의미
- 각 특성은 데이터의 한 측면을 나타내며, 이들이 모여 다차원 공간을 형성

### 차원 축소의 필요성
1. **데이터 크기 감소**: 저장 공간과 처리 시간 절약
2. **노이즈 제거**: 중요하지 않은 특성 제거로 모델 성능 향상
3. **시각화**: 고차원 데이터를 2D 또는 3D로 표현 가능
4. **다중공선성 해결**: 특성 간 상관관계 감소

## 주성분 분석 (PCA) 원리

PCA는 다음 원칙을 기반으로 작동:

1. **분산 최대화**: 데이터의 분산이 가장 큰 방향을 찾음
2. **직교성**: 각 주성분은 서로 직교(수직)
3. **순차적 최적화**: 첫 번째 주성분이 가장 중요하며, 순차적으로 중요도가 감소

### PCA 과정
1. 데이터 중심화 (평균을 0으로 조정)
2. 공분산 행렬 계산
3. 고유값과 고유벡터 계산
4. 주성분 선택 (가장 큰 고유값에 해당하는 고유벡터)
5. 데이터를 새로운 축으로 투영

## 차원 축소 방법

### 1. 선형 투영
- PCA가 대표적인 선형 투영 방법
- 고차원 데이터를 저차원 평면에 투영
- 예: 3D 공간의 데이터를 2D 평면에 투영

### 2. 매니폴드 학습
- 비선형 차원 축소 기법
- 데이터의 내재된 구조를 보존하며 차원 축소
- 주요 알고리즘:
  - t-SNE (t-distributed Stochastic Neighbor Embedding)
  - UMAP (Uniform Manifold Approximation and Projection)

## PCA의 장단점

### 장점
- 차원 축소로 계산 효율성 증가
- 노이즈 감소 및 과적합 위험 감소
- 데이터 시각화 용이

### 단점
- 비선형 관계 포착 어려움
- 주성분의 해석이 어려울 수 있음
- 일부 중요한 정보 손실 가능성

## 결론

PCA는 고차원 데이터를 다루는 데 효과적인 도구 

데이터의 본질적인 구조를 유지하면서 차원을 줄여, 복잡한 데이터셋을 더 쉽게 분석하고 시각화

그러나 비선형 관계가 중요한 경우에는 매니폴드 학습과 같은 비선형 기법을 고려해야함

----

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_openml
from sklearn.decomposition import PCA
mnist = fetch_openml('mnist_784', version=1)
mnist.data
```

```python
mnist.data.iloc[0].values.reshape(28, 28)
# 흑백 이미지로 출력
plt.imshow(mnist.data.iloc[2].values.reshape(28, 28), cmap='gray_r')
plt.title(mnist.target[2])
plt.show()
```

![image](https://github.com/user-attachments/assets/83e7842c-e5b4-423e-9f06-551489fd43c8)


```python
import seaborn as sns
iris = sns.load_dataset('iris')
X = iris.drop('species', axis=1)
pca = PCA(n_components=2)
pca.fit(X)
X_PCA = pca.transform(X)
X_PCA

iris = pd.DataFrame(X_PCA)
iris['species'] = iris.index
sns.scatterplot(x=0, y=1, hue='species', data=iris)
```

![image](https://github.com/user-attachments/assets/eac8a073-336d-4efc-9fd4-87f4f78107b1)


```python
# 적절한 주성분 계수 찾기 (분산이 95% 적당) + 90%
pca = PCA()
pca.fit(mnist.data)
# 설명된 분산의 누적의 합이 0.95이상이 되는 차원의 개수
cumsum = np.cumsum(pca.explained_variance_ratio_)
n_components_90 = np.argmax(cumsum >= 0.90) + 1
n_components_95 = np.argmax(cumsum >= 0.95) + 1
print(n_components_90, n_components_95)
d = np.argmax(cumsum >= 0.95) + 1
print('선택할 차원 수 :', d)

# 87 154
# 선택할 차원 수 : 154
```

```python
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(mnist.data)
print(X_pca.shape)
# (70000, 154)
```
```python
# cunsum을 시각화하는 코드
plt.plot(cumsum)
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()
```

![image](https://github.com/user-attachments/assets/a73fa7f5-7497-4344-a4cb-3f95decc8015)

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_validate
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

rfc = RandomForestClassifier(random_state=42)
x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target, stratify=mnist.target, random_state=42)

rfc.fit(x_train, y_train)
print(rfc.score(x_train, y_train))
print(rfc.score(x_test, y_test))

# 1.0
# 0.9677714285714286
```
```python
from sklearn.metrics import classification_report
y_pred = rfc.predict(x_test)
print(classification_report(y_test, y_pred))
```

![image](https://github.com/user-attachments/assets/d3875e14-e403-4d33-ba62-7e2dd54f9752)


----

# PCA의 변형: 랜덤 PCA와 점진적 PCA

주성분 분석(PCA)의 효율성을 높이기 위한 두 가지 주요 변형 기법인 랜덤 PCA와 점진적 PCA

## 랜덤 PCA

랜덤 PCA는 확률적 알고리즘을 사용하여 PCA의 계산 속도를 향상시키는 방법

### 특징
- **확률적 접근**: 근사값을 사용하여 계산 속도 향상
- **매개변수 설정**: `svd_solver='randomized'`로 지정
- **적용 조건**: 특성 수가 많고 원하는 주성분 수가 적을 때 효과적

### 사용 방법
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=k, svd_solver='randomized')
```

### 자동 선택
- scikit-learn의 PCA에서 `svd_solver='auto'`가 기본값
- 다음 조건에서 자동으로 랜덤 PCA 적용:
  1. 특성 수 > 500
  2. `n_components` < 특성 수의 80%

### 주의사항
- 모든 상황에서 항상 빠른 것은 아님
- 정확한 결과가 필요한 경우 `svd_solver='full'` 사용

## 점진적 PCA (Incremental PCA)

점진적 PCA는 대용량 데이터셋을 처리할 때 메모리 효율성을 높이는 방법

### 특징
- **미니배치 처리**: 데이터를 작은 배치로 나누어 처리
- **메모리 효율성**: 전체 데이터셋을 한 번에 메모리에 로드할 필요 없음
- **온라인 학습**: 새로운 데이터에 대해 모델 업데이트 가능

### 사용 방법
```python
from sklearn.decomposition import IncrementalPCA

ipca = IncrementalPCA(n_components=k)
for batch in data_batches:
    ipca.partial_fit(batch)
```

### 장점
1. **메모리 사용량 감소**: 대용량 데이터셋 처리 가능
2. **실시간 처리**: 스트리밍 데이터에 적용 가능
3. **유연성**: 새로운 데이터로 모델 지속적 업데이트 가능

### 주의사항
- 배치 크기 선택이 성능에 영향을 줄 수 있음
- 전체 데이터셋을 한 번에 처리하는 것보다 결과가 약간 다를 수 있음

## 결론

랜덤 PCA와 점진적 PCA는 각각 계산 속도와 메모리 효율성을 개선하여 PCA의 적용 범위를 넓힘.

데이터의 특성과 계산 환경에 따라 적절한 방법을 선택하면 더 효율적인 차원 축소가 가능.

----

```python
from sklearn.decomposition import IncrementalPCA
n_batch = 100
ipca = IncrementalPCA(n_components=5)
for data in np.array_split(mnist.data, n_batch):
    ipca.partial_fit(data)
# 병렬 학습 이후에 transform을 이용해서 데이터 변환
ipca_data = ipca.transform(mnist.data)
print(ipca_data.shape)
# (70000, 5)
```

```python
!wget https://bit.ly/fruits_300_data
fruits = np.load('fruits_300_data')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.decomposition import PCA
pca = PCA(n_components=50)
# 50개의 주성분으로 설정
pca.fit(fruits_2d)

print(pca.components_.shape)
# (50, 10000)

```

```python
import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):
    n = len(arr)
    rows = int(np.ceil(n/10))
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10+j < n:
                axs[i, j].imshow(arr[i*10+j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()
```
```python
draw_fruits(pca.components_.reshape(-1, 100, 100))
```

![image](https://github.com/user-attachments/assets/a7ecf63f-5c97-4d40-9e1f-2d74164f5ecb)


```puython
print(fruits_2d.shape)
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)

# (300, 10000)
# (300, 50)
```

```python
# 원본 데이터 재구성
fruits_inverse = pca.inverse_transform(fruits_pca)
# 50개의 특성으로 원본 데이터의 상당 부분을 재구성 가능
print(fruits_inverse.shape)

# (300, 10000)
```

```python
fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)
for start in [0,100,200]:
    draw_fruits(fruits_reconstruct[start:start+100])
    print("\n")
```

```python
# 설명된 분산
print(np.sum(pca.explained_variance_ratio_))
# 92%로 높은 분산을 유지 -> 원본 데이터를 복원했을 때 이미지의 품질이 높은 이유

plt.plot(pca.explained_variance_ratio_)
plt.show()
```

![image](https://github.com/user-attachments/assets/4953fad9-71b6-40f1-b08c-b8353eb0cad7)

```python
# 다른 알고리즘과 함께 사용하기

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

target = np.array([0]*100 + [1]*100 + [2]*100)

from sklearn.model_selection import cross_validate
# 교차검증
scores = cross_validate(lr, fruits_2d, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 0.9966666666666667
# 0.8869451999664306

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 0.9966666666666667
# 0.011748027801513673

pca = PCA(n_components=0.5)
pca.fit(fruits_2d)
# 주성분의 개수
print(pca.n_components_)

fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 2
# (300, 2)
# 0.9933333333333334
# 0.032831764221191405

```

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)
print(np.unique(km.labels_, return_counts=True))
# (array([0, 1, 2], dtype=int32), array([110,  99,  91]))
```

```python
for label in range(0, 3):
    data = fruits_pca[km.labels_ == label]
    plt.scatter(data[:, 0], data[:, 1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```

![image](https://github.com/user-attachments/assets/8f560fa7-73e4-427c-80a8-a83f2705405b)

## 정리

- 차원 축소는 원본 데이터의 특성을 적은 수의 새로운 특성으로 변환하는 비지도 학습의 한 종류
    - 저장 공간을 줄이고 시각화하기 쉬움.
    - 다른 알고리즘의 성능을 높일 수 있음

- 주성분 분석은 차원 축소 알고리즘의 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법
    - 원본 데이터를 주성분에 투영하여 새로운 특성을 만들 수 있음
    - 일반적으로 주성분은 원본 데이터에 있는 특성 개수보다 작음

- 설명된 분산은 주성분 분석에서 주성분이 얼마나 원본 데이터의 분산을 잘 나타내는지 기록
    - 사이킷런의 PCA 클래스는 주성분 개수나 설명된 분산의 비율을 지정하여 주성분 분석을 수행
 
----

## 실습

1차 목표 (베이스 모델)

- 결측치는 제거, 베이스 모델을 구축해서(RandomForestClassifier) classification_report 성능 평가

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import OrdinalEncoder

# 결측치 제거
df.dropna(inplace=True)

# 특성과 라벨 분리
X = df.drop('income', axis=1)
y = df['income']

# OrdinalEncoder를 사용하여 범주형 변수 인코딩
oe = OrdinalEncoder()
X[X.select_dtypes(include=['object']).columns] = oe.fit_transform(X.select_dtypes(include=['object']))

# Train, Test 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.transform(X_test)

rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)

print(classification_report(y_test, y_pred))

```
![image](https://github.com/user-attachments/assets/e71166e5-a9ed-4eeb-9940-d9e0adc8ec1d)

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# 데이터 로드
path = '/content/drive/MyDrive/Colab Notebooks/encore/csv/adult.csv'
df = pd.read_csv(path)

# 불필요한 열 제거
df = df.drop(['education', 'relationship'], axis=1)

# 결측치 처리
df[df == '?'] = np.nan
for col in df.columns:
    if df[col].isna().sum() > 0:
        df[col] = df[col].fillna(df[col].mode()[0])

# 중복 제거
df = df.drop_duplicates()

# 범주형 변수 인코딩
oe = OrdinalEncoder()
category_cols = df.select_dtypes(include=['object', 'category']).columns
for col in category_cols:
    df[col] = oe.fit_transform(df[[col]])

# 상관관계 시각화
plt.figure(figsize=(15, 5))
sns.heatmap(df.corr(), annot=True)
plt.show()

# 특성과 타겟 분리
X = df.drop('income', axis=1)
y = df['income']

# PCA 적용
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X)

# SMOTE를 사용한 오버샘플링
smote = SMOTE(random_state=42)
X_pca_smote, y_pca_smote = smote.fit_resample(X_pca, y)

# 훈련/테스트 세트 분리
X_train, X_test, y_train, y_test = train_test_split(X_pca_smote, y_pca_smote, 
                                                    stratify=y_pca_smote, random_state=42)

# 스케일링
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.transform(X_test)

# 랜덤 포레스트 모델 학습 및 평가
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
```
