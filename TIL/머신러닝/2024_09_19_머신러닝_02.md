```python
import matplotlib.pyplot as plt

bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/fe204451-695b-4d92-83d2-619e32f82f25)

```python
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/0d9b4879-7067-4e8f-a16f-c88ed0464f32)

```python
# k-최근접 이웃
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
# zip() 함수와 리스트 내포 구문 -> 2차원 리스트
fist_data = [[l, w] for l, w in zip(length, weight)]
print(fist_data)
```

## 정리

- **특성**은 데이터를 표현하는 하나의 성질

- **훈련** : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정

  - 사이킷런에서는 fit() 메서드가 하는 역할

- **k-최근점 이웃 알고리즘**은 가장 간단한 머신러닝 알고리즘

  - 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부

- 머신러닝 프로그램에서는 알고지름이 구현된 객체를 **모델**이라고 부른다

  - 종종 알고리즘 자체를 모델이라고도 부름

- **정확도**는 정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값

  - 사이킷런에서는 0~1 사이의 값으로 출력

  - 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)

-------

```python
# 지도 학습과 비지도 학습
# 지도 학습에서 데이터와 정답을 입력과 타깃이라 부름
# 이를 훈련 데이터라고 함

from sklearn.neighbors import KNeighborsClassifier

fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]
fish_target = [1]*35 + [0]*14
kn = KNeighborsClassifier()
print(fist_data[4])
print(fish_data[0:5])
print(fish_data[:5])
print(fish_data[44:])


# [29.0, 430.0]
# [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]]
# [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]]
# [[12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
```

```python
train_input = fish_data[:35]
train_target = fish_target[:35]
test_input = fish_data[35:]
test_target = fish_target[35:]

kn.fit(train_input, train_target)
kn.score(test_input, test_target)

# 샘플링 편향 문제
# 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않아 발생하는 문제

#0
```

```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
print(index)

# [13 45 47 44 17 27 26 25 31 19 12  4 34  8  3  6 40 41 46 15  9 16 24 33
#  30  0 43 32  5 29 11 36  1 21  2 37 35 23 39 10 22 18 48 20  7 42 14 28
#  38]

```

```python
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

import matplotlib.pyplot as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/89f90470-71dd-45b3-ad5a-bb57a4e37ff5)

## 정리

- **지도 학습**은 입력과 타깃을 전달하여 모델을 훈련한 다음 새로운 데이터를 예측하는 데 활용 -> k-최근접 이웃이 지도 학습 알고리즘

- **비지도 학습**은 타깃 데이터가 없다. 따라서 무엇을 예측하는 것이 아니라 입력데이터에서 어떤 특징을 찾는 데 주로 활용

- **훈련 세트**는 모델을 훈련할 때 사용하는 데이터. 보통 훈련세트가 클수록 좋으며 테스트 세트를 제외한 모든 데이터를 사용

- **테스트 세트**는 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많음. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있음

## numpy

- seed()는 난수를 생성하기 위한 정수 초깃값

- arange()는 일정한 간격의 정수 또는 실수 배열
  - 기본 간격은 1

- shuffle()은 주어진 배열을 랜덤하게 섞음

------

```python
# 데이터 전처리

fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

import numpy as np

np.column_stack([[1, 2, 3], [4, 5, 6]]) # 튜플

fish_data = np.column_stack((fish_length, fish_weight))
print(fish_data[:5])

# [[ 25.4 242. ]
#  [ 26.3 290. ]
#  [ 26.5 340. ]
#  [ 29.  363. ]
#  [ 29.  430. ]]

```

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)

print(train_input.shape)
print(test_input.shape)

print(train_target.shape)
print(test_target.shape)

print(test_target) # 0의 비율이 현저히 적은 것을 확인

# (36, 2)
# (13, 2)
# (36,)
# (13,)
# [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

```

```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)
print(test_target) # 0의 개수가 하나 늘은 상황
# stratify 매개변수에 타겟 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다.

# [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]

```

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```

```python
import matplotlib.pyplot as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/def1cc4f-c681-4c9c-821a-76be9f0ce72a)

```python
distances, indexes = kn.kneighbors([[25, 150]])

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/bdd97b38-7c06-40c6-a66c-cf0ee6ad37a6)

```python
# x출과 y축의 범위가 다른 상황

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# 두 특성이 놓인 범위가 매우 다름 -> 이를 두 특성의 스케일이 다르다 라고 표현
# 데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. 특히 거리 기반일 경우
# 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준에 맞춰 줘야함. -> 데이터 전처리
```

![image](https://github.com/user-attachments/assets/b16de89c-1ea3-47c3-b53a-486f68c193de)

```python
# 가장 널리 사용하는 전처리 방법 -> 표준점수 (standard score)
# 혹은 z점수라고도 부름

mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

print(mean, std)

# [ 27.29722222 454.09722222] [  9.98244253 323.29893931]
```

```python
# 전처리 데이터로 모델 훈련하기
train_scaled = (train_input - mean) / std

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/7cb35314-bedc-4018-b75d-0df8860ccbc2)

```python
new = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/a679a5d4-cefb-44e2-81f6-61b7a9f00db1)

```python
kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean) / std
kn.score(test_scaled, test_target)
# 1.0
```

```python
distances, indexes = kn.kneighbors([new])
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/7ea99238-0135-40e6-9284-12b1ccd06484)

## 정리
- 데이터 전처리는 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계로 많은 시간이 소모되기도 한다.

- 표준 점수는 훈련 세트의 스케일을 바꾸는 대표적인 방법
  - 표준 점수를 얻으려면 특성의 평균을 빼고 표준편차로 나눈다.
  - 반드시 훈련 세트의 평균과 표준편차로 테스트 세트를 바꿔야한다.

- 브로드캐스팅은 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능

##  패키지와 함수

- train_test_split() 훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수
  - stratify 매개변수에 클래스 레이블이 담긴 배열을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다.

- kneighbors()는 k-최근점 이웃 객체의 메서드
  - 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환


-----


```python
# k-최근접 이웃 회귀 k-최근접 이웃 회귀

import numpy as np
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])


import matplotlib.pyplot as plt
plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/de7aa4df-b7ae-4bc3-9500-974b1d66c348)


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

test_array = np.array([1,2,3,4])
print(test_array.shape)

test_array = test_array.reshape(2,2)
print(test_array.shape)

# (4,)
# (2, 2)
```

```python
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
print(train_input.shape, test_input.shape)

# (42, 1) (14, 1)
```

```python
# 결정계수(R^2)
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
print(knr.score(test_input, test_target))

# 0.992809406101064
```

```python
from sklearn.metrics import mean_absolute_error

test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
print(mae)

# 19.157142857142862
```

## 정리

- 회귀는 임의의 수치를 예측하는 문제. 타깃값도 임의의 수치

- k-최근접 이웃 회귀는 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값의 평균하여 예측

- 결정계수(R^2)는 대표적인 회귀 문제의 성능 측정 도구
  - 1에 가까울수록 좋고, 0에 가깝다면 성능이 나쁜 모델

- 과대적합은 모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어남
  - 모델이 훈련세트에 너무 집착해서 데이터에 내재도니 거시적인 패턴을 감지하지 못함

- 과소적합은 이와 반대로 모두 동일하게 낮거나 테스트 세트 성능이 오히려 높을 때 일어남

------

```python
# K-최근접 이웃의 한계

import numpy as np
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor

train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)

knr = KNeighborsRegressor(n_neighbors = 3)
knr.fit(train_input, train_target)
print(knr.predict([[50]]))

# [1033.33333333]
```

```python
import matplotlib.pyplot as plt

distances, indexes = knr.kneighbors([[50]])

plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')
plt.scatter(50, 1033, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/user-attachments/assets/da784a45-b775-4f4c-be02-8bebd3d860bc)


```python
print(np.mean(train_target[indexes]))

# 1033.3333333333333

distances, indexes = knr.kneighbors([[100]])

plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')
plt.scatter(100, 1033, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

```

![image](https://github.com/user-attachments/assets/8e8de059-f282-4841-b58b-f60864834f97)


```python
# 선형 회귀
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(train_input, train_target)
print(lr.predict([[50]]))

# [1241.83860323]

print(lr.coef_, lr.intercept_)

# [39.01714496] -709.0186449535477

plt.scatter(train_input, train_target)
plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])

plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/4db89923-bc81-4cd3-beca-91623b7170f7)

```python
# 다항 회귀
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

lr = LinearRegression()
lr.fit(train_poly, train_target)

point = np.arange(15,50)
plt.scatter(train_input, train_target)
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)

plt.scatter(50, 1574, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![image](https://github.com/user-attachments/assets/077c552a-f190-4887-8ea4-2aa0cda89e98)


```python
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))

# 0.9706807451768623
# 0.9775935108325122
```

## 정리

- 선형 회귀는 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾음
  - 특성이 하나면 직선 방정식이 된다.

- 선형 회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 계수 또는 가중치에서 저장된다.
  - 머신러닝에서 종종 가중치는 방정식의 기울기와 절편 모두 의미하는 경우가 많음

- 모델 파라미터는 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터를 의미

- 다항 회귀는 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다.
  - 이 함수는 비선형일 수 있지만 여전히 선형 회귀로 표현 가능
 
------


```python
# 다중 회귀
# 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업 - 특성 공학
import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv_data')
perch_full = df.to_numpy()

perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])


train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)

poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)

# (42, 9)

poly.get_feature_names_out()

# array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2',
#        'x2^2'], dtype=object)

```

```python
# 다중회귀모델 훈련하기
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))

# 0.9903183436982125
# 0.9714559911594111
```

```python
poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape)
# (42, 55)

lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))

# 0.9999999999996433
# -144.40579436844948
```

```python
# 규제
# 머신러닝 모델이 훈련 세트를 과도하게 학습하지 못하도록 훼방하는 것
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

# 릿지 회귀
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

# 0.9896101671037343
# 0.9790693977615387

```

```python
# 사람이 직접 지정해야 하는 매개변수 - 하이퍼파라미터

import matplotlib.pyplot as plt
train_score = []
test_score = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    ridge = Ridge(alpha=alpha)
    ridge.fit(train_scaled, train_target)
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

![image](https://github.com/user-attachments/assets/4a31aba7-5139-4862-bc78-a76bbceaf0ea)

```python
# 10^(-1) 즉 0.1에서 최댓값을 확인할 수 있다.
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

# 0.9903815817570367
# 0.9827976465386928
```

```python
# 라쏘 회귀
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

# 0.989789897208096
# 0.9800593698421883
```

```python
train_score = []
test_score = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(train_scaled, train_target)

    train_score.append(lasso.score(train_scaled, train_target))
    test_score.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

![image](https://github.com/user-attachments/assets/5f74d94f-aea3-441e-95e5-a2c46d9ed3c7)


```python
# 10^(1) 에서 test 값이 최대가 되는 것을 확인
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

# 0.9888067471131867
# 0.9824470598706695

```python
# 라쏘 모델은 계수 값을 아예 0으로 만들 수 있음
print(np.sum(lasso.coef_ == 0))
# 즉 라쏘 모델에게 유용한 특성은 55- 40 =15개 인 것을 확인
```

## 정리

- **다중 회귀**는 여러 개의 특성을 사용하는 회귀 모델로 특성이 많은면 선형 모델은 강력한 성능을 발휘

- **특성 공학**은 주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정이다.

- **릿지**는 규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 작게 만들어 과대적합을 완화
  - 비교적 효과가 좋아 널리 사용하는 규제 방법

- **라쏘**는 또 다른 규제가 있는 선형 회귀 모델로 릿지와 달리 계수 값을 아예 0으로 만들 수 있음

- **하이퍼파라미터**는 머신러닝 알고리즘이 학습하지 않는 파라미터
  - 사람이 사전에 지정해줘야함
  - 대표적으로 릿지와 라쏘의 규제 강도 alpha 파라미터
 
------

```python
# 로지스틱 회귀
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```

![image](https://github.com/user-attachments/assets/5326a860-57a0-4a63-ba62-ef924d18abb6)

```python
print(pd.unique(fish['Species']))

# ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
```

```python
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
print(fish_input[:5])
fish_target = fish['Species'].to_numpy()

# [[242.      25.4     30.      11.52     4.02  ]
#  [290.      26.3     31.2     12.48     4.3056]
#  [340.      26.5     31.1     12.3778   4.6961]
#  [363.      29.      33.5     12.73     4.4555]
#  [430.      29.      34.      12.444    5.134 ]]
```

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

# 0.8907563025210085
# 0.85

```

시그모이드 함수

```python
# 로지스틱 회귀
# 시그모이드 함수 (로지스틱 함수)
import numpy as np
import matplotlib.pyplot as plt
z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))
plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```

![image](https://github.com/user-attachments/assets/908a8f97-22fb-49be-8461-31ca728be3f0)


```python
# 로지스틱 회귀로 이진 분류 수행
# 넘파이 배열은 True, False 값을 전달하여 행을 선택 가능 -> 불리언 인덱싱

char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

# ['A' 'C']

```

```python
# Bream과 Smelt 만 True로 설정
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

# 로지스틱 회귀로 다중 분류 수행하기
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

proba = lr.predict_proba(test_scaled[:5])
```

## 정리

- 로지스틱 회귀는 선형 방정식을 사용한 분류 알고리즘
  - 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여클래스 확률을 출력할 수 있다.

- 다중 분류는 타킷 클래스가 2개 이상인 분류 문제
  - 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용하여 클래스를 예측

- 시그모이드 함수는 선형 방정식의 출력을 0과 1사이의 값으로 압축하여 이진 분류를 위해 사용한다.

- 소프트맥스 함수는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다.


--------

### 계단 함수

- 입력데이터를 0 과 1로 만 출력

### 시그모이드 함수

- 선형회귀 출력값을 0 ~ 1 사이로 변경 --> 확률로 해석

- x는  y = wx +b

- 클래스 1에 속할 확률이 p(x) = 0.8

- 임계치가 존재... 임계치 이상이면 1 아니면 0  보통은 0.5

### 손실함수(로그손실)

- log loss, cross entropy

- 실제 값과 예측값의 차이를 나타내는 함수 -> 차이를 최소화 하는 방향으로 학습

- 경사 하강법 - 최적화 : 손실함수의 값이 최소가 되는 방향을 찾아서 원래 가중치를 갱신

### 학습 횟수 : epoch

- 에포크 : 전체데이터를 한번 사용해서 학습

### 학습률 : learning late

- 한번에 경사를 내려가는 보폭의 크기
