# 비지도 학습 (Unsupervised Learning)

비지도 학습은 머신 러닝의 한 분야로, 레이블이 지정되지 않은 데이터에서 패턴과 구조를 발견하는 방법

## 특징

- 정답(레이블)이 없는 데이터에서 패턴을 발견
- 데이터에 대한 사전 지식이 부족할 때 유용
- 데이터의 숨겨진 구조를 찾는 알고리즘

## 장점

1. 레이블이 필요 없음: 데이터 준비 과정이 상대적으로 간단
2. 숨겨진 패턴 발견: 예상치 못한 인사이트 제공 가능
3. 대량의 데이터 활용 가능: 레이블이 없는 데이터도 사용 가능

## 주요 알고리즘

### 1. 군집화 (Clustering)

군집화는 유사한 특성을 가진 데이터를 그룹으로 묶는 기법입니다.

#### 주요 알고리즘:
- K-평균 군집화
- 계층적 군집화
- DBSCAN

#### 활용 사례:
- 고객 세분화
- 문서 분류
- 이미지 분류
- 유전자 군집화

### 2. 차원 축소 (Dimensionality Reduction)

차원 축소는 데이터의 특성 수를 줄이면서 중요한 정보를 유지하는 기법

#### 주요 알고리즘:
- 주성분 분석 (PCA)
- t-SNE
- 오토인코더

#### 목적:
- 데이터 시각화
- 노이즈 제거
- 계산 복잡성 감소

### 3. 이상치 탐지 (Anomaly Detection)

정상적인 패턴에서 벗어난 데이터 포인트를 식별하는 기법

#### 활용 사례:
- 금융 사기 탐지
- 시스템 오류 감지

## 비지도 학습의 활용

1. 탐색적 데이터 분석
2. 고객 세분화
3. 추천 시스템
4. 이미지 인식

## 지도 학습과의 비교

| 특성 | 비지도 학습 | 지도 학습 |
|------|------------|----------|
| 데이터 레이블 | 없음 | 있음 |
| 피드백 | 없음 | 있음 |
| 목적 | 데이터 구조 이해 | 예측 및 분류 |
| 복잡성 | 높음 | 상대적으로 낮음 |
| 정확도 | 상대적으로 낮음 | 높음 |

비지도 학습은 데이터의 숨겨진 구조를 발견하는 데 유용하며, 특히 레이블이 없는 대량의 데이터를 다룰 때 효과적

그러나 결과의 해석이 어려울 수 있고, 지도 학습에 비해 정확도가 낮을 수 있음

-----

```python
# 군집 알고리즘
!wget https://bit.ly/fruits_300_data

import numpy as np
import matplotlib.pyplot as plt


fruits = np.load('fruits_300_data')
print(fruits.shape)

# (300, 100, 100)

print(fruits[0, 0, :])

# [  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1
#    2   2   2   2   2   2   1   1   1   1   1   1   1   1   2   3   2   1
#    2   1   1   1   1   2   1   3   2   1   3   1   4   1   2   5   5   5
#   19 148 192 117  28   1   1   2   1   4   1   1   3   1   1   1   1   1
#    2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
#    1   1   1   1   1   1   1   1   1   1]

```

```python
plt.imshow(fruits[0], cmap='gray')
plt.show()
```

![image](https://github.com/user-attachments/assets/4e51cec6-b60e-495f-97de-834497143672)

```python
plt.imshow(fruits[0], cmap='gray_r')
plt.show()
```

![image](https://github.com/user-attachments/assets/413bc1ef-dca0-45eb-88a3-982371b617df)

```python
fig, axs = plt.subplots(1, 2, figsize=(10, 5))

axs[0].imshow(fruits[100], cmap='gray_r')
axs[0].set_title('Image 100')  # 제목 추가
axs[0].axis('off')  # 축 제거 (선택사항)

# 두 번째 이미지 표시
axs[1].imshow(fruits[200], cmap='gray_r')
axs[1].set_title('Image 200')  # 제목 추가
axs[1].axis('off')  # 축 제거 (선택사항)

# 이미지 보여주기
plt.show()
```

![image](https://github.com/user-attachments/assets/72a2b460-aaa9-490b-bbbc-974165811542)

```python
# 픽셀값 분석하기
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

print(apple.shape)
# (100, 10000)
```

```python
plt.hist(np.mean(apple, axis=1), alpha = 0.8)
plt.hist(np.mean(pineapple, axis =1), alpha = 0.8)
plt.hist(np.mean(banana, axis =1), alpha = 0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
```

![image](https://github.com/user-attachments/assets/9e444d56-43c4-4952-b294-e3ba29c8f213)

```python
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
```

![image](https://github.com/user-attachments/assets/d73d5048-6f73-4d07-8472-4c9697a6660e)

```python
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```

![image](https://github.com/user-attachments/assets/978203ab-0b61-42bd-80a4-4763fa8769f5)

```python
# 평균값과 가까운 사진 고르기
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1,2))
print(abs_mean.shape)

# (300,)
```

```python
apple_index = np.argsort(abs_mean)[:100]
fig, axs = plt.subplots(10, 10, figsize=(10, 10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```

![image](https://github.com/user-attachments/assets/f51bd439-ab1e-43f5-91e5-21a88f0e9dd2)

## 정리

- **비지도 학습**은 머신러닝의 한 종류로 훈련 데이터에 타깃이 없다.
- 타깃이 없기 때문에 외부의 도움 없이 스스로 유용한 무언가를 학습해야 함
- 대표적으로 군집, 차원 축소 등이 있음.

- **히스토그램**은 구간별로 값이 발생한 빈도를 그래프로 표시
- 보통 x축이 값의 구간(계급)이고 y축은 발생 빈도(도수)

- **군집**은 비슷한 샘플끼리 하나의 그룹으로 모으는 대표적인 비지도 학습 작업
- 군집 알고리즘으로 모은 샘플 그룹을 클러스터라고 부름

-----

# K-평균 군집화 (K-Means Clustering)

K-평균 군집화는 가장 널리 사용되는 비지도 학습 알고리즘 중 하나로, 데이터를 K개의 그룹으로 나누는 방법.

## 알고리즘 개요

1. 군집의 수 K를 사전에 설정합니다.
2. K개의 중심점(centroid)을 무작위로 초기화합니다.
3. 각 데이터 포인트를 가장 가까운 중심점에 할당합니다.
4. 각 군집의 새로운 중심점을 계산합니다 (할당된 포인트들의 평균).
5. 중심점이 더 이상 변하지 않거나 최대 반복 횟수에 도달할 때까지 3-4 단계를 반복합니다.

## 주요 특징

- 간단하고 빠른 알고리즘
- 대용량 데이터에도 적용 가능
- 구형(spherical) 클러스터에 적합
- 이상치에 민감할 수 있음

## 장단점

### 장점
- 이해하기 쉽고 구현이 간단함
- 대규모 데이터셋에 효율적

### 단점
- 최적의 K 값을 사전에 알아야 함
- 초기 중심점 선택에 따라 결과가 달라질 수 있음
- 비선형적으로 분리된 데이터에는 적합하지 않음

## K 값 선택 방법

- 엘보우 방법 (Elbow method)
- 실루엣 분석 (Silhouette analysis)
- 교차 검증 (Cross-validation)

## Python 예시 코드

```python
from sklearn.cluster import KMeans
import numpy as np

# 데이터 생성
X = np.random.rand(100, 2)

# K-means 모델 생성 및 학습
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# 클러스터 레이블 및 중심점 확인
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

print("클러스터 레이블:", labels)
print("중심점:", centroids)
```

K-평균 군집화는 데이터 세분화, 이미지 압축, 이상 탐지 등 다양한 분야에서 활용되는 유용한 알고리즘

-----

```python
# k-평균
!wget https://bit.ly/fruits_300_data

import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300_data')
fruits_2d = fruits.reshape(-1, 100*100)
```

```python
# 사이킷런은 2차원 데이터를 요구
# 원본 데이터는 300,100,100 --> 300,10000
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)
```

```python
print(np.unique(km.labels_, return_counts=True))
# (array([0, 1, 2], dtype=int32), array([112,  98,  90]))
```

```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```

![image](https://github.com/user-attachments/assets/97df3ff2-5360-4a86-9820-985e8b90dba0)

```python
print(km.transform(fruits_2d[100:101]))
# [[3400.24197319 8837.37750892 5279.33763699]]
```

```python
# 최적의 k 찾기
inertia = []
for k in range(2, 7):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)
plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```

![image](https://github.com/user-attachments/assets/d7e2f5e4-a323-4ef8-8d20-f2e6bd694df9)

## 정리

- **k-평균 알고리즘**은 처음에 랜덤하게 클러스터 중심을 정하고 클러스터를 생성
- 이후 클러스터의 중심을 이동하고 다시 클러스터를 만드는 식으로 반복해서 최적의 클러스터를 구성하는 알고리즘

- 클러스터 중심은 k-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값.
- **센트로이드**라고도 불림
- 가장 가까운 클러스터 중심을 샘플의 또 다른 특성으로 사용하거나 새로운 샘플에 대한 예측으로 활용할 수 있음.

- **엘보우 방법**은 최적의 클러스터 개수를 정하는 방법 중 하나.
- 이너셔는 클러스터 중심과 샘플 사이 거리의 제곱 합.
- 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있음.

-----

# DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

DBSCAN은 밀도 기반의 클러스터링 알고리즘으로, 데이터의 분포 형태에 구애받지 않고 효과적으로 군집을 찾아낼 수 있는 방법

## 주요 특징

1. 밀도 기반 클러스터링
2. 사전에 클러스터 수를 지정할 필요 없음
3. 임의의 형태의 클러스터 탐지 가능
4. 노이즈와 이상치 처리에 강함

## 핵심 매개변수

1. **epsilon (ε)**: 이웃을 정의하는 반경
2. **min_samples**: 핵심 포인트로 간주되기 위한 최소 이웃 수

## 알고리즘 작동 원리

1. 임의의 포인트에서 시작
2. 해당 포인트의 ε-이웃(epsilon 반경 내 포인트들)을 찾음
3. 이웃의 수가 min_samples 이상이면 해당 포인트를 핵심 포인트로 지정하고 새로운 클러스터 시작
4. 핵심 포인트의 ε-이웃에 있는 모든 포인트를 같은 클러스터에 포함
5. 새롭게 클러스터에 추가된 포인트에 대해 2-4 과정을 반복
6. 더 이상 새로운 포인트를 추가할 수 없을 때까지 클러스터 확장
7. 아직 방문하지 않은 포인트에 대해 1-6 과정을 반복

## 포인트 분류

- **핵심 포인트**: ε-이웃 내 포인트 수가 min_samples 이상인 포인트
- **경계 포인트**: 핵심 포인트는 아니지만 핵심 포인트의 ε-이웃에 속하는 포인트
- **노이즈 포인트**: 핵심 포인트도 아니고 경계 포인트도 아닌 포인트

## DBSCAN vs K-means

| 특성 | DBSCAN | K-means |
|------|--------|---------|
| 클러스터 수 지정 | 불필요 | 필요 |
| 클러스터 형태 | 임의의 형태 가능 | 구형 또는 볼록한 형태 |
| 이상치 처리 | 강함 | 약함 |
| 대규모 데이터셋 | 효율적 | 상대적으로 비효율적 |
| 밀도가 다른 클러스터 | 잘 처리함 | 잘 처리하지 못함 |

## 장점

1. 클러스터 수를 사전에 지정할 필요가 없음
2. 임의의 형태의 클러스터 탐지 가능
3. 이상치 탐지에 효과적
4. 대규모 데이터셋에 적용 가능

## 단점

1. 밀도가 다양한 클러스터가 있는 데이터셋에서는 성능이 저하될 수 있음
2. 고차원 데이터에서는 "차원의 저주" 문제로 성능이 떨어질 수 있음
3. epsilon과 min_samples 매개변수 선택이 결과에 큰 영향을 미침

## Python 구현 예시

```python
from sklearn.cluster import DBSCAN
import numpy as np

# 데이터 생성
X = np.random.rand(100, 2)

# DBSCAN 모델 생성 및 학습
dbscan = DBSCAN(eps=0.3, min_samples=5)
clusters = dbscan.fit_predict(X)

# 결과 출력
print(f"클러스터 레이블: {np.unique(clusters)}")
print(f"노이즈 포인트 수: {sum(clusters == -1)}")
```

DBSCAN은 복잡한 형태의 클러스터를 찾아내는 데 탁월하며, 특히 이상치 탐지나 노이즈가 많은 데이터셋에서 유용하게 사용 가능

-----

# 실루엣 분석 (Silhouette Analysis)

실루엣 분석은 클러스터링 품질을 평가하는 방법으로, 각 데이터 포인트가 자신의 클러스터에 얼마나 잘 맞는지 측정

## 실루엣 점수

- 범위: -1 ~ 1
- 의미:
  - 1에 가까울수록: 클러스터에 잘 속함
  - 0에 가까울수록: 클러스터 경계에 위치
  - -1에 가까울수록: 잘못된 클러스터에 할당됨

## 실루엣 계수 계산

```
s(i) = (b(i) - a(i)) / max(a(i), b(i))
```

- a(i): 같은 클러스터 내 평균 거리 (응집도)
- b(i): 가장 가까운 다른 클러스터와의 평균 거리 (분리도)

## 평균 실루엣 점수 해석

- 0.5 이상: 잘 분리된 클러스터링
- 0.25 ~ 0.5: 클러스터 구조가 약함
- 0.25 미만: 클러스터링 결과가 좋지 않음

실루엣 분석은 최적의 클러스터 수 결정과 클러스터링 품질 평가에 유용한 도구
