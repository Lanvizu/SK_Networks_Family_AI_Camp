# 확률적 경사 하강법

```python
# 확률적 경사 하강법
# SGDClassifier
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

# 특성 전처리
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 0.773109243697479
# 0.775
```

```python
# 1에포크씩 이어서 훈련
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 0.8487394957983193
# 0.9
```

```python
import numpy as np
sc = SGDClassifier(loss='log_loss', random_state=42)
train_score = []
test_score = []

classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))

import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![image](https://github.com/user-attachments/assets/143089c7-b2b2-4d68-a09d-a39969e1b8c5)


```python
# 일정 에포크 동안 성능이 향상되지 않으면 얼리 스탑핑
# tol 매개변수를 None으로 지정시 자동으로 멈추지 않고 max_iter=100 만큼은 무조건 반복

sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 0.957983193277311
# 0.925

sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 0.9495798319327731
# 0.925

```

## 정리

- 확률적 경사 하강법 : 훈련세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.
  - 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법
  - 한 번에 전체 샘플을 사용하면 배치 경사 하강법

- 손실 함수 : 확률적 경사 하강법이 최적화할 대상
  - 대부분의 문제에 잘 맞는 손실 함수가 이미 정의
  - 이진 분류 -> 로지스틱 회귀(또는 이진 크로스엔트로피) 손실 함수
  - 다중 분류 -> 크로스엔트로피 손실 함수
  - 회귀 문제 -> 평균 제곱 오차 손실 함수

- 에포크 : 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미
  - 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복

- SGDClassifier : 확률적 경사 하강법을 사용한 분류 모델을 생성
  - loss 매개변수로 최적화할 손실 함수를 지정
    - 기본은 hinge, 로지스틱 회귀는 'log_loss'
  - penalty는 규제의 종류 지정
  - max_iter은 에포크 횟구 지정
  - tol은 반복을 멈출 조건
 
------
