# 랜덤 포레스트 (Random Forest)

랜덤 포레스트는 여러 개의 결정 트리를 결합한 앙상블 학습 방법입니다. 이 기법은 다양한 트리들의 예측을 종합하여 더 정확하고 안정적인 예측을 수행합니다.

## 주요 특징

### 앙상블 학습
- 여러 개의 모델(결정 트리)을 결합하여 예측 성능을 높임
- 각 트리의 예측 결과를 **투표 방식**으로 종합하여 최종 예측을 수행

### 다수의 독립적 결정 트리
- 각 트리는 훈련 데이터의 서브셋을 사용하여 독립적으로 학습
- 트리 간 다양성 확보로 모델의 일반화 능력 향상

### 부트스트랩 샘플링
- 원본 데이터에서 **중복을 허용**하여 랜덤하게 샘플링
- 각 트리마다 서로 다른 훈련 데이터 사용

### 특성 무작위 선택
- 노드 분할 시 전체 특성 중 일부만 무작위로 선택
- 선택된 특성 중에서 최적의 분할 기준 결정
- 트리 간 다양성 증가 및 과적합 위험 감소

## 장점과 단점

### 장점

1. **과적합 방지**: 여러 트리의 결합으로 개별 트리의 과적합 영향 감소
2. **높은 정확도**: 다양한 트리의 앙상블로 예측 성능 향상
3. **특성 중요도 제공**: 각 특성의 예측 기여도 평가 가능
4. **데이터 노이즈에 강인**: 랜덤 샘플링과 특성 선택으로 노이즈 영향 감소
5. **비선형 관계 학습**: 복잡한 패턴과 상호작용 포착 가능

### 단점

1. **계산 비용**: 다수의 트리 생성으로 학습 및 예측 시간 증가
2. **메모리 사용량 증가**: 여러 트리 모델 저장에 많은 메모리 필요
3. **해석의 어려움**: 개별 트리에 비해 모델 전체의 의사결정 과정 파악 어려움

## 주요 하이퍼파라미터

| 파라미터 | 설명 |
|----------|------|
| `n_estimators` | 생성할 트리의 수 |
| `max_depth` | 각 트리의 최대 깊이 |
| `min_samples_split` | 노드 분할에 필요한 최소 샘플 수 |
| `max_features` | 각 분할에서 고려할 특성의 수 |

랜덤 포레스트는 다양한 머신러닝 작업에서 높은 성능을 보이며, 특히 **분류**와 **회귀** 문제에 널리 사용

적절한 하이퍼파라미터 튜닝을 통해 모델의 성능을 최적화 가능


```python
# 데이터 확보
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 0.9973541965122431 0.8905151032797809
```

```python
rf.fit(train_input, train_target)
print(rf.feature_importances_)

rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)

# [0.23167441 0.50039841 0.26792718]
# 0.8934000384837406
```

```python
# 엑스트라 트리 : 랜덤 포레스트와 달리 부트스트랩 샘플을 사용하지 않음
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 0.9974503966084433 0.8887848893166506
```

```python
et.fit(train_input, train_target)
print(et.feature_importances_)

# [0.20183568 0.52242907 0.27573525]
```
```python
# 그레이디언트 부스팅 : 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식
# 결정 트리의 개수를 늘려도 과대적합에 매우 강함.
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 0.8881086892152563 0.8720430147331015
```

```python
# 결정 트리 개수 500개로 증가 -> 과대 적합 억제 중
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9464595437171814 0.8780082549788999
```
```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)
# [0.15887763 0.6799705  0.16115187]
```
```python
# 히스토그램 기반 그레이디언트 부스팅
# 정형 데이터를 다루는 머신러닝 알고리즘에 특화
# 입력 특성을 256개의 구간으로 나눔
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9321723946453317 0.8801241948619236
```
```python
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)
print(result.importances_mean)
# [0.08876275 0.23438522 0.08027708]
```
```python
result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)
print(result.importances_mean)
# [0.05969231 0.20238462 0.049     ]
```
```python
hgb.score(test_input, test_target)
# 0.8723076923076923
```
```python
# 다양한 라이브러리
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9558403027491312 0.8782000074035686
```
```python
from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.935828414851749 0.8801251203079884
```

## 정리

- 앙상블 학습은 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘
- 랜덤 포레스트는 대표적인 결정 트리 기반의 앙상블 학습 방법
    - 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만듬
- 엑스트라 트리는 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않음
    - 대신 랜덤하게 노드를 분할해 과대적합을 감소
- 그레이디언트 부스팅은 랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법.
    - 훈련 속도가 조금 느리지만 더 좋은 성능 기대
    - 그레이디언트 부스팅의 속도를 개선한 것이 히스토그램 기반 그레이디언트 부스팅

-----

```python
# 강사님 버전
from sklearn.datasets import load_wine
wine = load_wine()
print(wine.data.shape)
print(wine.target.shape)

from sklearn.model_selection import train_test_split
x = wine.data
y = wine.target
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, random_state=42)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
rf.fit(x_train, y_train)
print(rf.score(x_train, y_train))
print(rf.score(x_test, y_test))

# 1.0
# 1.0
```

```python
from sklearn.metrics import classification_report
pred = rf.predict(x_test)
print(classification_report(y_test, pred))
```

![image](https://github.com/user-attachments/assets/7e855993-d4bc-455b-8660-8afd116f648f)


## 교차 검증 (Cross-Validation)

머신러닝 모델의 성능은 데이터의 분할 방식에 따라 달라질 수 있습니다. 단일 분할로는 모델의 일반화 능력을 정확히 평가하기 어려우므로, 교차 검증을 통해 더 신뢰할 수 있는 성능 평가를 수행합니다.

### 교차 검증의 필요성

- 데이터 분할에 따른 성능 변동 최소화
- 모델의 일반화 능력 더 정확히 평가
- 과적합 위험 감소

### K-폴드 교차 검증 (K-Fold Cross-Validation)

1. 전체 데이터를 K개의 부분집합(폴드)으로 나눔
2. K번의 학습-평가 과정 수행:
   - K-1개의 폴드로 모델 학습
   - 남은 1개의 폴드로 모델 평가
3. K번의 성능 측정 결과의 평균을 최종 성능으로 사용

예: 5-폴드 교차 검증
```
데이터: [폴드1] [폴드2] [폴드3] [폴드4] [폴드5]

1차: [테스트] [학습] [학습] [학습] [학습]
2차: [학습] [테스트] [학습] [학습] [학습]
3차: [학습] [학습] [테스트] [학습] [학습]
4차: [학습] [학습] [학습] [테스트] [학습]
5차: [학습] [학습] [학습] [학습] [테스트]

최종 성능 = (성능1 + 성능2 + 성능3 + 성능4 + 성능5) / 5
```

### 장점

1. **신뢰성 향상**: 여러 번의 평가로 더 안정적인 성능 추정
2. **데이터 활용 극대화**: 모든 데이터가 학습과 평가에 사용됨
3. **과적합 감지**: 학습 성능과 검증 성능의 차이를 통해 과적합 여부 판단 가능

### 단점

1. **계산 비용**: K번의 모델 학습으로 시간과 자원 소모 증가
2. **하이퍼파라미터 튜닝 복잡성**: 교차 검증과 결합 시 튜닝 과정이 더 복잡해짐

교차 검증은 모델의 성능을 더 정확하게 평가하고 일반화 능력을 향상시키는 중요한 기법

특히 데이터셋의 크기가 작거나 불균형한 경우에 유용하며, 대부분의 머신러닝 프로젝트에서 필수적으로 사용

```python
# cross-validate
from sklearn.model_selection import cross_validate
scores = cross_validate(rf, x_train, y_train, return_train_score=True, n_jobs=-1) # 기본이 5폴드, n_jobs : 모든 리소스를 최대한으로 사용
print(scores)
print(np.mean(scores['train_score']))
print(np.mean(scores['test_score']))
```


# 앙상블 학습 (Ensemble Learning)

앙상블 학습은 여러 개의 모델을 결합하여 더 강력하고 정확한 예측 모델을 만드는 기법

## 학습기 유형

### 약한 학습기 (Weak Learner)
- 성능이 랜덤 추측보다 약간 좋은 정도의 모델
- 결합하면 성능이 크게 향상됨
- 예: 깊이가 1인 결정 트리, 단순 선형 모델, 기본 kNN

### 강한 학습기 (Strong Learner)
- 개별적으로 우수한 성능을 보이는 학습기
- 여러 약한 학습기를 결합하여 생성 가능
- 예: 랜덤 포레스트, SVM, 인공신경망

## 앙상블 기법

### 1. 보팅 (Voting)
- 여러 모델의 예측 결과를 투표를 통해 결합
- 분류: 다수결 투표 / 회귀: 평균값 사용

### 2. 배깅 (Bagging)
- Bootstrap Aggregating의 줄임말
- 여러 약한 학습기를 병렬로 학습 후 결과 결합
- 대표적 예: 랜덤 포레스트

### 3. 부스팅 (Boosting)
- 약한 학습기를 순차적으로 학습
- 이전 모델의 오차를 보완하는 방향으로 다음 모델 학습
- 주요 알고리즘:
  - AdaBoost (Adaptive Boosting)
  - Gradient Boosting

### 4. 스태킹 (Stacking)
- 여러 모델의 예측을 결합하여 최종 예측 생성
- 강한 학습기와 약한 학습기 모두 사용 가능
- 일반적 구조:
  1. 여러 기본 모델(주로 약한 학습기) 학습
  2. 기본 모델의 예측을 입력으로 사용하는 메타 모델(주로 강한 학습기) 학습

## 앙상블 학습의 목적

앙상블 학습의 주요 목적은 여러 약한 학습기를 결합하여 강한 학습기를 생성

- 모델의 일반화 능력 향상
- 과적합 위험 감소
- 예측 안정성 증가
- 복잡한 패턴 학습 능력 향상

앙상블 기법은 다양한 머신러닝 대회에서 우수한 성능을 보이며, 실제 문제 해결에도 널리 사용

-----

### 실습

1차원 LabelEncoder를 사용한 버전.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from ucimlrepo import fetch_ucirepo 
  
# fetch dataset 
mushroom = fetch_ucirepo(id=73) 
  
# data (as pandas dataframes) 
X = mushroom.data.features 
y = mushroom.data.targets 

# 랜덤 시드 고정
random_state = 42

# 원본 데이터 복사
X_copy = X.copy()

# LabelEncoder 초기화 및 적용
encoders = {}

for column in X_copy.columns:
    le = LabelEncoder()
    # nan 값을 문자열 'nan'으로 변경하여 라벨 인코딩에 포함
    X_copy[column] = X_copy[column].fillna('nan')
    X_copy[column] = le.fit_transform(X_copy[column])
    encoders[column] = le

# stalk-root 열에 대한 인코더 저장
stalk_root_encoder = encoders['stalk-root']

# nan 값을 가진 행을 제외한 데이터셋 생성
new_x = X_copy[X_copy['stalk-root'] != stalk_root_encoder.transform(['nan'])[0]].copy()

# stalk-root 타입을 새로운 y 값으로 지정
new_y = new_x['stalk-root']
new_x.drop('stalk-root', axis=1, inplace=True)

# train, test 분리
x_train, x_test, y_train, y_test = train_test_split(new_x, new_y, stratify=new_y, random_state=random_state)

# 모델 생성 및 학습
rf = RandomForestClassifier(random_state=random_state)
rf.fit(x_train, y_train)

# nan 값 예측
nan_mask = X_copy['stalk-root'] == stalk_root_encoder.transform(['nan'])[0]
X_nan = X_copy[nan_mask].drop('stalk-root', axis=1)
y_pred = rf.predict(X_nan)

# 예측값으로 nan 값 채우기
X_copy.loc[nan_mask, 'stalk-root'] = y_pred

# 전체 데이터에 대한 X와 y 준비
le = LabelEncoder()
y = le.fit_transform(y)
X = X_copy

# 다시 train, test 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=random_state)

# 새로운 모델 생성 및 학습
rf_final = RandomForestClassifier(random_state=random_state)
rf_final.fit(X_train, y_train)

# 모델 성능 평가
print("\n## 최종 모델 성능:")
print("Train Score:", rf_final.score(X_train, y_train))
print("Test Score:", rf_final.score(X_test, y_test))

# 분류 보고서 출력
y_pred = rf_final.predict(X_test)
print("\n## 분류 보고서:")
print(classification_report(y_test, y_pred))

```

![image](https://github.com/user-attachments/assets/95978e06-64ce-43bb-a67f-79369a4d9076)


2차원 OrdinalEncoder를 사용한 버전.

```python
from ucimlrepo import fetch_ucirepo
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd

# 데이터셋 불러오기
mushroom = fetch_ucirepo(id=73)
X = mushroom.data.features
y = mushroom.data.targets

# 'stalk-root' 열 분리 및 결측치 처리
na_x = X.drop('stalk-root', axis=1)
na_y = X['stalk-root']

# OrdinalEncoder를 사용하여 전체 데이터 인코딩
oe = OrdinalEncoder()
na_x_encoded = oe.fit_transform(na_x)
na_y_encoded = oe.fit_transform(na_y.values.reshape(-1, 1))

# 결측치가 없는 데이터로 모델 학습
not_null_index = na_y.notnull()
x_train = na_x_encoded[not_null_index]
y_train = na_y_encoded[not_null_index].ravel()

na_model = RandomForestClassifier(random_state=42)
na_model.fit(x_train, y_train)

# 결측치가 있는 데이터 예측 및 채우기
null_index = na_y.isnull()
x_test = na_x_encoded[null_index]
preds = na_model.predict(x_test)
na_y_encoded[null_index] = preds.reshape(-1, 1)

# 인코딩된 'stalk-root' 열을 원본 데이터에 추가
na_x_encoded_full = pd.DataFrame(na_x_encoded, columns=na_x.columns)
na_x_encoded_full['stalk-root'] = na_y_encoded

# 전체 데이터셋으로 학습 및 평가
X_train, X_test, y_train, y_test = train_test_split(
    na_x_encoded_full, y, stratify=y, random_state=42)

model1 = RandomForestClassifier(random_state=42)
model1.fit(X_train, y_train)

# 모델 성능 평가
print("Test Score:", model1.score(X_test, y_test))
print("\n## 분류 보고서:")
print(classification_report(y_test, model1.predict(X_test)))
```

![image](https://github.com/user-attachments/assets/52dad2e9-5cd5-4b1b-b5fa-dc94b8c44d17)
