- 결정 트리
- 분류 회귀
- 입력 데이터를 여러 기준으로 분류, 각 분할이 순순한 (동일한 클래스)의 하위 집합을 만들도록 설계된 알고리즘
- 노드(node) 데이터의 특정 특징에 대한 조건을 나타낸다. --> 말단 노드가 최종 분류
    - 루트 노드
    - 내부 노드
    - 말단 노드
- 데이터 분할 기준 - 분류 문제
    - 정보 이득
        - 엔트로피의 차이를 이용
    - 지니 계수
        - 특정 클래스에서 클래스가 얼마나 잘 분리되는지를 측정하는 지표

    - 적용 가능한 데이터
        - 정보 이득(엔트로피 기반) : 다수의 클래스를 분류
            - 의류진단 데이터
            - 이상탐지 데이터
        - 지니 계수 : 이진 분류
    - 분산 감소
        - 회귀 문제 사용
        - 노드에 있는 데이터들의 분산을 최소화하는 방향
    - 각 노드의 분할 기준을 선정하기 때문에 해당 모델을 학습 시 치퍼들간의 중요도를 파악할 수 있음

-----

```python
from sklearn.datasets import load_wine
wine = load_wine()

import pandas as pd
df = pd.DataFrame(wine.data, columns=[
    'Alcohol',
    'Malic acid',
     'Ash',
     'Alcalinity of ash',
     'Magnesium',
     'Total phenols',
     'Flavanoids',
     'Nonflavanoid phenols',
     'Proanthocyanins',
     'Color intensity',
     'Hue',
     'OD280/OD315 of diluted wines',
     'Proline'
])
df['target'] = wine.target
df.head()

df.target.value_counts()

```

![image](https://github.com/user-attachments/assets/d5643f28-c353-4855-84b3-db5703a6c53f)

```python
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion= 'entropy',random_state=42)

x = df.drop('target', axis=1)
y = df.target

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, random_state=42)

tree.fit(x_train, y_train)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(12, 10))
plot_tree(tree, filled=True, feature_names=x_train.columns)
plt.show()
```

![image](https://github.com/user-attachments/assets/2a795d36-0887-43c3-a8a3-6b7d0e7198f3)


```python
tree.score(x_test, y_test)

from sklearn.metrics import classification_report
pred = tree.predict(x_test)
print(classification_report(y_test, pred))
```

![image](https://github.com/user-attachments/assets/c1481c78-aec8-4f1d-9d8a-a78740f3f25f)


## 정리

 - **결정 트리**는 예 / 아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘
    - 비교적 예측 과정을 이해하기 쉽고 성능도 뛰어남

- **불순도**는 결정 트리가 최적의 질문을 찾기 위한 기준
    - 사이킷런은 지니 불순도와 엔트로피 불순도를 제공

- **정보 이득**은 부모 노드와 자식 노드의 불순도 차이
    - 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습

- 결정 트리는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉬움
    - **가지치기**는 결정 트리의 성장을 제한하는 방법
    - 사이킷런의 결정 트리 알고리즘은 여러 가지 가지치기 매개변수를 제공

- **특성 중요도**는 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값
    - 특성 중요도를 계산할 수 있는 것이 결정 트리의 또 다른 큰 장점

-----

# 클래스 불균형 해결 방법

- 언더샘플링
    - 다수 클래스의 샘플을 무작위로 제거하여 소수 클래스의 수에 맞춤
    - 장점: 데이터 처리 시간 감소
    - 단점: 중요한 특성 소실 가능성, 정보 손실 위험

- 오버샘플링
    - 소수 클래스의 샘플을 증가시켜 다수 클래스와 균형을 맞춤
    - 방법: 복제 또는 병합
    - 단점: 과적합 발생 가능성

- SMOTE (Synthetic Minority Over-sampling Technique)
    - 소수 클래스의 새로운 샘플을 생성
    - 특징: k-최근접 이웃(k-NN) 알고리즘을 사용하여 새로운 샘플 생성
    - 과적합 방지에 도움
    - 단점: 실제 데이터 분포 왜곡 가능성 존재

- 앙상블 기법
    - 여러 모델을 결합하여 더 강력한 모델 생성
    - 예시: 랜덤 포레스트
    - 클래스 불균형 문제에 우수한 성능을 보임
    - 다양한 앙상블 기법들이 클래스 불균형 해결에 효과적

- 클래스 가중치 조정
    - 모델 학습 시 손실 함수에 클래스별 가중치 부여
    - 소수 클래스의 오류에 더 큰 패널티 부여
    - 불균형한 데이터셋에서 모델의 성능 향상에 도움
 

```python
pip install imblearn

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# 딕셔너리 형태로 나타내는 Counter
Counter(y_train)

# Counter({0: 44, 1: 53, 2: 36})

# 언더 샘플링
rds = RandomUnderSampler(random_state=42)
x_train_under, y_train_under = rds.fit_resample(x_train, y_train)
Counter(y_train_under)

# Counter({0: 36, 1: 36, 2: 36})

# 오버샘플링
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
x_train_over, y_train_over = ros.fit_resample(x_train, y_train)
Counter(y_train_over)

# Counter({0: 53, 1: 53, 2: 53})

# SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)
Counter(y_train_smote)

# Counter({0: 53, 1: 53, 2: 53})
```

```python
tree = DecisionTreeClassifier(random_state=42)
tree.fit(x_train_under, y_train_under)
print(classification_report(y_test, tree.predict(x_test)))
```

![image](https://github.com/user-attachments/assets/0ed42347-6c45-485b-a71e-72838a18b1a1)

## 교차 검증과 그리드 서치

```python
# 교차 검증과 그리드 서치

import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from operator import sub
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42
)

# train_input 와 train_target을 다시 train_test_split() 함수에 넣어 훈련 세트를 만든다.
sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42
)

print(sub_input.shape, val_input.shape)

# (4157, 3) (1040, 3)
```

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))

# 0.9971133028626413
# 0.864423076923077
```

```python
# 교차 검증
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)

# {'fit_time': array([0.06950092, 0.03105092, 0.03593755, 0.03679347, 0.0334816 ]), 'score_time': array([0.00988865, 0.0107038 , 0.00557971, 0.00237226, 0.0062921 ]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
```

```python
import numpy as np
print(np.mean(scores['test_score']))

# 0.855300214703487
```

```python
# 하이퍼파리미터 튜닝
# 그리드 서치를 사용

from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004,0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

dt = gs.best_estimator_
print(dt.score(train_input, train_target))
print(gs.best_params_)
print(gs.cv_results_['mean_test_score'])

best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])

# 0.9615162593804117
# {'min_impurity_decrease': 0.0001}
# [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]
# {'min_impurity_decrease': 0.0001}
```

```python
params = {'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001),
         'max_depth' : range(5,20,1),
         'min_samples_split' : range(2,100,10)}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))

# {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}
# 0.8683865773302731

```

```python
# 랜덤 서치
from scipy.stats import uniform, randint

rgen = randint(0,10)
rgen.rvs(10)

# array([5, 1, 4, 5, 9, 6, 3, 9, 9, 1])
```

```python
params = {'min_impurity_decrease' : uniform(0.0001, 0.001),
          'max_depth' : randint(20,50),
          'min_samples_split' : randint(2,25),
          'min_samples_leaf' : randint(1,25)}

from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)

print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))

# {'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}
# 0.8695428296438884

dt = gs.best_estimator_
print(dt.score(train_input, train_target))
# 0.8928227823744468

```

## 정리

- **검증 세트**는 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트

- **교차 검증**은 훈련 세트를 여러 폴드로 나눈 다음 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련
    - 교차 검증은 이런 식으로 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법

- **그리드 서치**는 하이퍼파라미터 탐색을 자동화해 주는 도구
    - 탐색할 매개변수를 나열하면 교차 검증을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택
    - 마지막으로 이 매개변수 조합으로 최종 모델을 훈련

- **랜덤 서치**는 연속된 매개변수 값을 탐색할 때 유용
    - 탐색할 값을 직접 나열하는 것이 아니고 탐색 값을 샘플링할 수 있는 확률 분포 객체를 전달.
    - 지정된 횟수만큼 샘플링하여 교차 검증을 수행하기 때문에 시스템 자원이 허락하는 만큼 탐색량을 조절 가능
 
-----

