```python
# Install gdown if not already installed
!pip install gdown

import gdown
import pandas as pd

# Google Drive file ID
file_id = '14bYukzdAEZiikPMFHH9V_xzcsobAUgS7'
# Create the download URL
download_url = f'https://drive.google.com/uc?id={file_id}'

# Download the file using gdown
gdown.download(download_url, 'data.csv', quiet=False)

# Load the CSV file into a DataFrame
df = pd.read_csv('data.csv')

# Display the first few rows of the DataFrame
print(df.head())
```

- 피처요약표
- 결측값 시각
- 결측값 처리
  - 많으면 제거
  - 많지않으면 대체
  - 결측값 자체가 예측에 도움이 되는 경우, 하나의 고유값으로 간주

- 머신러닝 모델 OOF 예측, LightGBM(마이크로소프트), XGBoost(부스팅 알고리즘, 결정트리를 직렬로 배치), 앙상블(여러개 조합)
- 피처엔지니어링 :
- 하이퍼 파라메터 최적화 : 베이지안 최적화
  - 베이지안 최적화 : 차원 축소와 같이 사용하면 효과

```python
# 시각화
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno

data = df.copy()

# 음수 데이터를 na로 치환
data.replace(-1, pd.NA, inplace=True)

summary = pd.DataFrame(data.dtypes, columns=['데이터 타입'])
summary['결측값 개수'] = data.isna().sum()
summary['고유값 개수'] = data.nunique().values
summary['데이터 종류'] = data.apply(lambda x: x.unique())
for indexName in summary.index:
    if 'bin' in indexName:
        summary.loc[indexName, '데이터 종류'] = '이진형'
    elif 'cat' in indexName:
        summary.loc[indexName, '데이터 종류'] = '명목형'
    elif summary.loc[indexName, '데이터 타입'] == 'float64':
        summary.loc[indexName, '데이터 종류'] = '연속형'
    elif summary.loc[indexName, '데이터 타입'] == 'int64':
        summary.loc[indexName, '데이터 종류'] = '순서형'
summary
```

![image](https://github.com/user-attachments/assets/87a247d6-d826-4206-b8a0-697b114d9fce)

```python
# 데이터 시각화
# 타겟의 분포 - 전략... 데이터의 편향 - %도 표시
# 이진 계열은 bar 형태로 시각화 유리

plt.pie(df['target'].value_counts(),
        labels=df['target'].value_counts().index,
        autopct='%.2f%%',)
plt.title('target 분포')
plt.show()
```

![image](https://github.com/user-attachments/assets/3b1cb464-210a-4ce2-be65-9d453df5f5d4)

```python
bin_feature = summary.loc[summary['데이터 종류'] == '이진형'].index
bin_feature
```

![image](https://github.com/user-attachments/assets/8d9e1cb6-2c29-4845-a137-4c555f2a4b9a)

```python
# 시각화 함수
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import matplotlib as mpl

def plot_target_ratio_by_features(df, features, num_rows, num_cols, size = (12, 19)):
    mpl.rc('font', size=9)
    fig = plt.figure(figsize=size)
    grid = gridspec.GridSpec(ncols=num_cols, nrows=num_rows)
    plt.subplots_adjust(hspace=0.3, wspace=0.3)

    for i, feature in enumerate(features):
        ax = plt.subplot(grid[i])
        sns.barplot(data=df, x=feature, y='target', ax=ax, palette='Set2')
    plt.show()
```

```python
# 6행 3열
fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(15, 20))
axes = axes.flatten()
for i, feature in enumerate(bin_feature):
    sns.barplot(data=df, x=feature, y='target', ax=axes[i])
    axes[i].set_title(f'{feature} - target')
plt.tight_layout()
plt.show()
# sns.barplot(data=df, x='target', y=bin_feature[0])

```

![image](https://github.com/user-attachments/assets/cfe426da-9284-467f-a147-921dfa629616)

![image](https://github.com/user-attachments/assets/59212995-e3d5-402f-ac96-1bf063626589)

```python
# 명목형, 연속형 및 데이터 종류별로 시각화 함수를 통해서 시각화
# 상관 관계 : 연속형 데이터
# 결측치 제거 : heatmap
plt.figure(figsize=(7, 7))
bin_feature = summary.loc[summary['데이터 종류'] == '연속형'].index
corr = df[bin_feature].corr()
sns.heatmap(corr, cmap='OrRd', annot=True)
plt.show()
```

![image](https://github.com/user-attachments/assets/cb55afcd-a2c3-425d-bb25-784ac6fb3b48)

```python
# 베이스라인 모델
X = df.drop(columns=['id','target'])
y = df['target']

# 카테고리 형태의 명목형 피처는 onehot 인코딩
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
cat_feature = summary.loc[summary['데이터 종류'] == '명목형'].index
X_cat = ohe.fit_transform(X[cat_feature])
X_cat
```

```python
# 피처제거
drop_feature = cat_feature

X =X.drop(columns=drop_feature)
X = X.reset_index(drop=True)
X.info()
```

```python
# X와 X_cat 합친다.
X.shape, X_cat.shape

all_data = pd.concat([X, pd.DataFrame(X_cat.toarray())], axis=1)
all_data.shape

# from sklearn.model_selection import train_test_split
# x, x_test, y, y_test = train_test_split(all_data, y, test_size=0.2, random_state=42)
num_train = int(len(all_data)*0.8)
x = all_data[:num_train]
x_test = all_data[num_train:]
y = y[:num_train]
y_test = y[num_train:]

# OOF 방식으로 LightGBM 훈련
# 교차 검증에서 사용되는 방법

from sklearn.model_selection import StratifiedKFold
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
import numpy as np
oof_val_preds = np.zeros(x.shape[0])
oof_test_preds = np.zeros(x_test.shape[0])
```
```python
import lightgbm as lgb
params = {
    'objective': 'binary',
    'learning_rate': 0.01,
    'force_row_wise': True,
    'random_state': 0
}

from sklearn.metrics import roc_auc_score
for idx,(train_idx, valid_idx) in enumerate(folds.split(x,y)):
  print("#"*40,f'폴드 {idx+1} / 폴드{folds.n_splits}', '#'*40)
  # 훈련용 데이터 ,검증용 데이터
  x_train,y_train = x.iloc[train_idx], y.iloc[train_idx]
  x_valid, y_valid = x.iloc[valid_idx], y.iloc[valid_idx]
  # light gbm 전용 데이터셋
  dtrain = lgb.Dataset(x_train, y_train)
  dvalid = lgb.Dataset(x_valid, y_valid)
  # 모델 훈련
  lgb_model = lgb.train(params, dtrain,
                        valid_sets=dvalid ,
                        callbacks=[lgb.early_stopping(10)] # Use early_stopping callback
                        )
  # 모델 저장
  lgb_model.save_model(f'lgbm_model_{idx+1}.txt')
  # roc_auc_score 측정
  valid_preds = lgb_model.predict(x_valid)
  score = roc_auc_score(y_valid, valid_preds)
  print(f'ROC AUC : {score}')

```

![image](https://github.com/user-attachments/assets/2e5b4360-f0af-49c6-b58c-fe296b70e635)


```python
# 성능 개선
# 피처엔지니어링 + 하이퍼 파라메터 튜닝
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
path = '/content/drive/MyDrive/Colab Notebooks/encore/csv/safe_drive.csv'
train = pd.read_csv(path)

all_data = train.drop('target', axis=1) # 타깃값 제거

all_features = all_data.columns # 전체 피처

from sklearn.preprocessing import OneHotEncoder

# 명목형 피처
cat_features = [feature for feature in all_features if 'cat' in feature]
# 원-핫 인코딩 적용

onehot_encoder = OneHotEncoder()
encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])

# '데이터 하나당 결측값 개수'를 파생 피처로 추가
all_data['num_missing'] = (all_data==-1).sum(axis=1)
```
```python
# 명목형 피처, calc 분류의 피처를 제외한 피처
remaining_features = [feature for feature in all_features
                      if ('cat' not in feature and 'calc' not in feature)]
# num_missing을 remaining_features에 추가
remaining_features.append('num_missing')

# 분류가 ind인 피처
ind_features = [feature for feature in all_features if 'ind' in feature]

is_first_feature = True
all_data['mix_ind']=''
for ind_feature in ind_features:
  all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'

cat_count_features = []
for feature in cat_features+['mix_ind']:
    val_counts_dict = all_data[feature].value_counts().to_dict()
    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x:
                                                           val_counts_dict[x])
    cat_count_features.append(f'{feature}_count')

```

```python
from scipy import sparse
# 필요 없는 피처들
drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',
                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']

# remaining_features, cat_count_features에서 drop_features를 제거한 데이터
all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)

# 데이터 합치기
all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),
                               encoded_cat_matrix],
                              format='csr')

num_train = len(train) # 훈련 데이터 개수

# 훈련 데이터와 테스트 데이터 나누기
X = all_data_sprs[:num_train]
X_test = all_data_sprs[num_train:]

y = train['target'].values

import numpy as np

def eval_gini(y_true, y_pred):
    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)
    assert y_true.shape == y_pred.shape

    n_samples = y_true.shape[0]                      # 데이터 개수
    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값

    # 1) 예측값에 대한 지니계수
    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬
    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선
    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수

    # 2) 예측이 완벽할 때 지니계수
    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬
    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선
    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수

    # 정규화된 지니계수
    return G_pred / G_true
```

```python
# LightGBM용 gini() 함수
def gini(preds, dtrain):
    labels = dtrain.get_label()
    return 'gini', eval_gini(labels, preds), True # 반환값

import lightgbm as lgb
from sklearn.model_selection import train_test_split

# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)
X_train, X_valid, y_train, y_valid = train_test_split(X, y,
                                                      test_size=0.2,
                                                      random_state=0)

# 베이지안 최적화용 데이터셋
bayes_dtrain = lgb.Dataset(X_train, y_train)
bayes_dvalid = lgb.Dataset(X_valid, y_valid)

```

```python
# 베이지안 최적화를 위한 하이퍼파라미터 범위
param_bounds = {'num_leaves': (30, 40),
                'lambda_l1': (0.7, 0.9),
                'lambda_l2': (0.9, 1),
                'feature_fraction': (0.6, 0.7),
                'bagging_fraction': (0.6, 0.9),
                'min_child_samples': (6, 10),
                'min_child_weight': (10, 40)}

# 값이 고정된 하이퍼파라미터
fixed_params = {'objective': 'binary',
                'learning_rate': 0.005,
                'bagging_freq': 1,
                'force_row_wise': True,
                'random_state': 1991}

def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,
                  bagging_fraction, min_child_samples, min_child_weight):
    '''최적화하려는 평가지표(지니계수) 계산 함수'''

    # 베이지안 최적화를 수행할 하이퍼파라미터
    params = {'num_leaves': int(round(num_leaves)),
              'lambda_l1': lambda_l1,
              'lambda_l2': lambda_l2,
              'feature_fraction': feature_fraction,
              'bagging_fraction': bagging_fraction,
              'min_child_samples': int(round(min_child_samples)),
              'min_child_weight': min_child_weight,
              'feature_pre_filter': False}
    # 고정된 하이퍼파라미터도 추가
    params.update(fixed_params)

    print('하이퍼파라미터:', params)

    # LightGBM 모델 훈련
    lgb_model = lgb.train(params=params,
                           train_set=bayes_dtrain,
                           num_boost_round=2500,
                           valid_sets=bayes_dvalid,
                           feval=gini,
                           callbacks=[lgb.early_stopping(300)],
                           )
    # 검증 데이터로 예측 수행
    preds = lgb_model.predict(X_valid)
    # 지니계수 계산
    gini_score = eval_gini(y_valid, preds)
    print(f'지니계수 : {gini_score}\n')

    return gini_score
```

```python
!pip install bayesian-optimization
from bayes_opt import BayesianOptimization

# 베이지안 최적화 객체 생성
optimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수
                                 pbounds=param_bounds, # 하이퍼파라미터 범위
                                 random_state=0)

# 베이지안 최적화 수행
optimizer.maximize(init_points=3, n_iter=6)

```

```python
# 평가함수 점수가 최대일 때 하이퍼파라미터
max_params = optimizer.max['params']
max_params
```

![image](https://github.com/user-attachments/assets/d6f8ab78-c092-4b11-865e-c2befbb47e1e)

```python
# 정수형 하이퍼파라미터 변환
max_params['num_leaves'] = int(round(max_params['num_leaves']))
max_params['min_child_samples'] = int(round(max_params['min_child_samples']))

# 값이 고정된 하이퍼파라미터 추가
max_params.update(fixed_params)
max_params

from sklearn.model_selection import StratifiedKFold

# 층화 K 폴드 교차 검증기 생성
folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)

# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열
oof_val_preds = np.zeros(X.shape[0])
# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열
oof_test_preds = np.zeros(X_test.shape[0])

# OOF 방식으로 모델 훈련, 검증, 예측
for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):
    # 각 폴드를 구분하는 문구 출력
    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)

    # 훈련용 데이터, 검증용 데이터 설정
    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터
    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터

    # LightGBM 전용 데이터셋 생성
    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋
    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋

    # LightGBM 모델 훈련
    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터
                          train_set=dtrain,     # 훈련 데이터셋
                          num_boost_round=2500, # 부스팅 반복 횟수
                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋
                          feval=gini,           # 검증용 평가지표
                          early_stopping_rounds=300, # 조기종료 조건
                          verbose_eval=100)     # 100번째마다 점수 출력

    # 테스트 데이터를 활용해 OOF 예측
    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits
    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측
    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)

    # 검증 데이터 예측확률에 대한 정규화 지니계수
    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])
    print(f'폴드 {idx+1} 지니계수 : {gini_score}\n')
```
