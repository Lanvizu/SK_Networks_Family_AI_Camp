# 데이터 사전 처리

## 누락 데이터 처리

분석 데이터의 품질에 의해서 데이터 분석의 정확도가 좌우됨

데이터 품질을 높이기 위해 누락데이터, 중복 데이터 등 오류를 수정하고 분석 목적에 맞게 변형하는 과정이 필요

일반적으로 유효한 데이터 값이 존재하지 않는 누락 데이터를 NaN으로 표시

머신러닝 분석 모형에 데이터를 입력하기 전 반드시 누락 데이터를 제거하거나 다른 적절한 값으로 대체하는 과정이 필요

누락 데이터 증가 -> 데이터 품질 감소 -> 머신러닝 분석 알고리즘을 왜곡하는 현상

### 누락 데이터 확인

```python
import seaborn as sns

df = sns.load_dataset('titanic')

df.head()
```
```python
# 데이터 정보확인 (타입, 인덱스 정렬 여부, 결측치 여부)
df.info()
```

```python
# deck 열의 NaN 개수 계산하기
# dropna = False를 사용해 누락 데이터의 개수를 확인 가능
df['deck'].value_counts(dropna=False)
```

![image](https://github.com/user-attachments/assets/4818078f-c16e-45aa-b17b-26c90bdb62d9)

```python
# 결측치 탐색
df.isna().sum()

# 결측치를 날려버리거나 다른 값으로 대체해야 하는데 쉽지않음
```

![image](https://github.com/user-attachments/assets/e5ed0bae-0622-4055-8c2a-80e2c3d35c4f)

```python
# isnull() 또는 isna() : 누락 데이터이면 True를 반환하고, 유효한 데이터가 존재하면 False를 반환한다.
# notnull() 또는 notna() : 유효한 데이터가 존재하면 True를 반환하고, 누락 데이터이면 False를 반환한다.
# 하지만 많은 데이터에서 각각의 열을 확인하기 어려우므로 sum을 사용한다.

df.head().isnull().sum(axis=0)

# sum메소드를 활용해 각 열의 누락 데이터 개수를 구한다.

df.isna().mean()

# 누락 데이터의 비율을 확인할 수 있다.
# 누락 데이터가 클수록 날려버리면 데이터의 거진 대부분이 날라가니까 대체하는 것이 중요하다...?
# 데이터 분석에서 가장 중요한 것은 도메인 지식(데이터의 특성)을 알아야한다 -> 정상적인 범위?
```


```python
# 피처 요약 리포트
import pandas as pd

dict_summary = {
    'Data type' : df.dtypes,
    'unique_data' : df.nunique(), # 데이터의 종류의 개수
    '0' : df.iloc[0,:],
    '1' : df.iloc[1,:],
    '2' : df.iloc[2,:]
}
# 평균이나 수치를 계산하기 어려운 부분을 처리하기 위해 numeric_only=True를 사용한다.
pd.DataFrame(dict_summary)

df.describe() # 기술 통계  기본적으로 수치데이터

df.describe(include='object') # object를 설정해 범주형 데이터

df.describe(include='all') # 모든 데이터
```

```python
sample1 = [True,True,True,False,True,True]
print(any(sample1)) # any : 한개라도 True가 있으면 전체를 True로 판단
print(all(sample1)) # all : 한개라도 False가 있으면 전체를 False로 판단 / 전부 True 값이여야 함

# True
# False
```

```python
# 펜사인덱스
df2 = df.head()['who']
temp_index = pd.Series([True,False,True,False,True])
df2[temp_index] # True에 해당하는 것만 출력된다.

```

```python
# row 중에 어느 항목이라도 na가 있으면 row를 출력
# 1. 전체 각각의 Row 데이터 중에서 각 컬럼별 isnaI()를 했을 때 True인 항목ㅇ이 있으면 해당 True로 설정한다. 그렇지 않으면 False
# 이런 형태로 구성된 시리즈를 만든다.
# 2. True False로 이루어진 시리즈를 데이터프레임의 [ ]에 넣는다.

df2 = df.drop(columns=['deck','age'])
df2[df2.isna().any(axis = 1)]
# 이런식으로 na가 있는 항복에 대해서는 해당 row를 출력해서 na와 주변 피처들의 관계를 살펴보고 어떻게 채울지 결정한다.


````
```python
# 데이터를 수집하면 피치의 특성을 파악하기 위한 사전 단계
# head() 모양 판단
# info() 결측치 여부 판단
# isna().sum() or isna().mean() 평균
# describe(), describe(include='object'), describe(include='all')
# 피처 요약 - 리포트

# missingno 라이브러리 활용
import missingno as msno
import matplotlib.pyplot as plt

# 매트릭스 그래프
msno.matrix(df)
plt.show()
```

![image](https://github.com/user-attachments/assets/1f33b9f1-e843-4bc0-b33c-485f0aea9d55)

```python
# 막대그래프
msno.bar(df)
plt.show()
```

![image](https://github.com/user-attachments/assets/afd739a3-887e-485e-8eae-5f18e6537f93)

```python
# 히트맵
msno.heatmap(df)
plt.show()
```

![image](https://github.com/user-attachments/assets/7d0df949-de67-4b29-8ab0-03aa0018f3d1)

```python
# 덴드로그램
msno.dendrogram(df)
plt.show()
```
![image](https://github.com/user-attachments/assets/19d1530e-f50f-4cd6-a6d0-bd78de2d67e1)


### 누락 데이터 제거

열을 삭제하면 분석 대상이 갖는 특성(변수)을 제거

행을 삭제하면 분석 대상의 관측값(레코드)을 제거

```python
# 각 컬럼별 na 개수 파악
temp = df.isna().sum() > 0
temp[temp>0]
```
![image](https://github.com/user-attachments/assets/d45ee183-d36c-4f9e-b784-3c9785a11235)

```python
df.dropna(axis=0)  # na가 있는 row 데이터는 전부 삭제 / 전체 데이터 개수가 줄어듦
df.dropna(axis=1)  # na가 있는 column 을 삭제 / 전체 데이터 개수가 동일하나 컬럼 수가 줄어듦
```

```python
# NaN 값이 500개 이상인 열을 모두 삭제 - deck 열(891개 중 688개의 NaN 값)
df2 = df.dropna(axis=1, thresh=500)
print(df2)
```


### 누락 데이터 대체

데이터 중 일부가 누락되어 있더라고 나머지 데이터를 최대한 살려 활용하면 좋은 결과


```python
# 1번째 누락 데이터 삭제
# age의 결측치에 해당하는 row데이터 삭제
df3 = df2.dropna(subset=['age'])
df3.shape
```

```python
# 2번째 누락 데이터 대체 -> 수치형은 평균 데이터로, 범추형은 최빈 데이터로
mean_age = df2['age'].mean().round()
df3.loc[:,'age'] = df2['age'].fillna(mean_age)
df3.isna().sum()
```

```python
# 범주형 데이터의 값을 채우는 방법
# 1. 가장 많이 발생하는 최빈값으로 채우기
import seaborn as sns

df = sns.load_dataset('titanic')

df['embark_town'][825:830]

most_freq = df['embark_town'].value_counts(dropna=True).idxmax()
print(most_freq)

most_freq2 = df['embark_town'].mode()[0] 

# most_freq2 = df['embark_town'].mode().values[0]
# mode라는 함수를 사용해 최빈값을 가져온다.
print(most_freq2)

df['embark_town'] = df['embark_town'].fillna(most_freq)
df['embark_town'][825:830]

```
![image](https://github.com/user-attachments/assets/dc2fbf01-fc0c-4ce4-85ff-9c09764f6504)

### 최종 처리 과정

```python

# 결측치를 다 채우기
# 전략
  # 수치형 데이터 : 평균
  # 범주형 : 최빈값 또는 주변값
    # 최빈값 : 결측기가 좀 많아서 주변값으로 채우기가 좀 힘들때
    # 주변값 : 소수의 결측치

df = sns.load_dataset('titanic')
df2 = df.copy()
df2.isna().sum()
df2.dropna(axis=1, thresh=500, inplace=True)
df2.isna().sum()

mean_age = df2['age'].mean().round()
print(mean_age)
df2.loc[:,'age'] = df2['age'].fillna(mean_age)
df2.isna().sum()

most_freq = df2['embark_town'].mode()[0]
print(most_freq)
df2['embark_town'] = df2['embark_town'].fillna(most_freq)
df2.isna().sum()

df2['embarked'] = df['embarked'].mode()[0]
df2.isna().sum()
```

```python
# 결측치 개수와 타입을 나타내는 함수
def reportNa(df):
    t = df.isna().sum()
    df_na = pd.DataFrame(t[t>0])
    df_na['type'] = df.dtypes[df_na.index]
    return df_na
```

```python
df = sns.load_dataset('taxis')
reportNa(df)

payment_preq = df['payment'].mode()[0]
df['payment'].fillna(payment_preq, inplace=True)

pickup_borough_preq = df['pickup_borough'].mode()[0]
df['pickup_borough'].fillna(pickup_borough_preq, inplace=True)

dropoff_borough_preq = df['dropoff_borough'].mode()[0]
df['dropoff_borough'].fillna(dropoff_borough_preq, inplace=True)

df['pickup_zone'].fillna(method='ffill', inplace=True)
df['dropoff_zone'].fillna(method='ffill', inplace=True)

reportNa(df)
```

## 중복 데이터 처리

### 중복 데이터 확인

동일한 관측값이 중복되는지 여부, 즉 행의 레코드가 중복되는지 여부를 확인하려면 duplicated() 메소드를 이용

전에 나온 행들과 비교하여 중복되는 행이면 True, 처음 나오면 False를 반환

```python
import pandas as pd

df = pd.DataFrame({'c1':['a','a','b','a','b'],
                   'c2':[1,1,1,2,2],
                   'c3':[1,1,2,2,2]})

# keep='first' 옵션이 기본 적용
# 2개의 중복행 중에서 처음 0행은 중복이 아니라고 판정
# 이후 중복행들은 모두 True로 표시

df_dup_first = df.duplicated()
df_dup_first
```

![image](https://github.com/user-attachments/assets/112e767a-bf91-4ddc-9350-f0d313e5baa4)

```python
# keep='last'
# 중복되는 행이면서 가장 마지막 행이 아닌 경우만 True로 판정.
df_dup_last = df.duplicated(keep='last')
df_dup_last
```
![image](https://github.com/user-attachments/assets/4c84da8f-7a8a-4f82-bc65-4ff3d5f6b9a7)

```python
# keep=False
# 데이터프레임에서 중되는 모든 행을 찾음
# 모든 중복되는 행을 True로 표현
df_dup_false = df.duplicated(keep=False)
df_dup_false
```
![image](https://github.com/user-attachments/assets/30339e39-44ef-41a2-ac8f-f7bc17afeef1)


### 중복 데이터 제거

데이터 수집, 데이터 통합, 시스템 오류 등 다양한 원인으로 발생

데이터의 품질을 보장하고 분석의 정확도를 높이는 데 매우 중요

```python
# keep='first' 기본 적용
# 첫 번째 행만을 남기고 나머지 중복 행들을 제거
df2 = df.drop_duplicates()
df2
```
![image](https://github.com/user-attachments/assets/e4f5e410-2428-4f9c-b07e-eb45cd3acbac)


```python
# keep='last'
# 마지막 행을 제외하고 나머지 행들을 제거
df3 = df.drop_duplicates(keep='last')
df3
```
![image](https://github.com/user-attachments/assets/05b0f924-80b5-4fd5-8aab-530e5e639e39)

```python
# keep=False
# 중복된 모든 행을 제거
df4 = df.drop_duplicates(keep=False)
df4
```
![image](https://github.com/user-attachments/assets/3b020835-6571-43be-b63d-e08710d7531e)

---

## 데이터 정규화

정확하게 정규화된 데이터는 분석 모델의 성능을 향상시키고 데이터 기반 의사 결정 과정에서 더 나은 결과를 도출할 수 있도록 도움

데이터 분석 과정에서 데이터 포맷을 일관성 있게 정규화하는 작업이 매우 중요

### 단위 환산

같은 데이터셋 안에서 서로 다른 측정 단위를 사용한다면 전체 데이터의 일관성 측면에서 문제가 발생

측정 단위를 동일하게 맞출 필요가 있음

### 자료형 변환

숫자가 문자열로 저장된 경우 숫자형으로 변환

## 범주형(카테고리) 데이터 처리

### 구간 분할

연속 데이터를 그대로 사용하기 보다 일정한 구건으로 나눠서 분석하는 것이 효율적인 경우가 있음

판다스의 cut() 함수를 이용하여 연속 데이터를 여러 구간으로 나누고 범주형 데이터로 변환

```python
from google.colab import drive
drive.mount('/content/drive')

import numpy as np

url = '/content/drive/MyDrive/Colab Notebooks/encore/csv/auto-mpg.csv'
df = pd.read_csv(url)
df.head()
```

![image](https://github.com/user-attachments/assets/f794b871-212e-4c8b-ac81-281c1d2906bd)

```python
# 열 이름을 지정
df.columns = ['mpg','cylinders','displacement','horsepower','weight',
              'acceleration','model year','origin','name'] 

# horsepower 열의 누락 데이터('?') 삭제하고 실수형으로 변환
df['horsepower'] = df['horsepower'].replace('?', np.nan)      # '?'을 np.nan으로 변경
df = df.dropna(subset=['horsepower'], axis=0)                 # 누락데이터 행을 삭제
df['horsepower'] = df['horsepower'].astype('float')      

print(df['horsepower'].dtypes)

# float64

print(df['origin'].unique())

df['origin'] = df['origin'].replace({1:'USA', 2:'EU', 3:'JPN'})
print(df['origin'].unique())
print(df['origin'].dtypes)

# ['USA' 'JPN' 'EU']
# ['USA' 'JPN' 'EU']
# object
```

```python
# origin 열의 문자열 자료형을 범주형으로 변환
df['origin'] = df['origin'].astype('category')
print(df['origin'].dtypes)

# 범부형을 문자열로 다시 변환
df['origin'] = df['origin'].astype('str')
print(df['origin'].dtypes)

# category
# object
```

```python
url = '/content/drive/MyDrive/Colab Notebooks/encore/csv/auto-mpg.csv'
df = pd.read_csv(url)

# 열 이름을 지정
df.columns = ['mpg','cylinders','displacement','horsepower','weight',
              'acceleration','model year','origin','name'] 

# horsepower 열의 누락 데이터('?') 삭제하고 실수형으로 변환
df['horsepower'] = df['horsepower'].replace('?', np.nan)      # '?'을 np.nan으로 변경
df = df.dropna(subset=['horsepower'], axis=0)                 # 누락데이터 행을 삭제
df['horsepower'] = df['horsepower'].astype('float')      

count, bin_dividers = np.histogram(df['horsepower'], bins=3)
print(bin_dividers)

# [ 46.         107.33333333 168.66666667 230.        ]
```

```python
bin_names = ['저출력', '보통출력', '고출력']

# pd.cut 함수로 각 데이터를 3개의 bin에 할당
df['hp_bin'] = pd.cut(x=df['horsepower'],
                      bins=bin_dividers,
                      labels=bin_names,
                      include_lowest=True)
df[['horsepower', 'hp_bin']].head(15)
```
![image](https://github.com/user-attachments/assets/293f1030-b9b0-4f4e-a7dd-2b589a5b5743)


### 더미 변수

컴퓨터가 인식 가능한 입력값으로 변환해야함

이 때 사용되는 숫자 0 또는 1로 표현하는 것을 더미 변수라고 함

어떤 특성이 있는지 없는지 여부만을 표시함

```python
# np.histogram 함수로 3개의 bin으로 구분할 경계값의 리스트 구하기
count, bin_dividers = np.histogram(df['horsepower'], bins=3)

bin_names = {'저출력', '보통출력', '고출력'}

df['hp_bin'] = pd.cut(x=df['horsepower'],
                      bins=bin_dividers,
                      labels=bin_names,
                      include_lowest=True)

horsepower_dummies = pd.get_dummies(df['hp_bin'])
print(horsepower_dummies)

horsepower_dummies_float = pd.get_dummies(df['hp_bin'], dtype=float)
print(horsepower_dummies_float)
```
![image](https://github.com/user-attachments/assets/df5264f4-e0d4-4e04-9ba0-4467fd8accd0)


```python
# 원핫인코딩
# sklearn 라이브러리 불러오기
from sklearn import preprocessing

# 전처리를 위한 encoder 객체 만들기
label_encoder = preprocessing.LabelEncoder()
onehot_encoder = preprocessing.OneHotEncoder()

# label encoder로 문자열 범주를 숫자형 범주로 변환
onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))
print(onehot_labeled)
print(type(onehot_labeled))

# 원핫인코딩을 위해 2차원 행렬로 형태 변경
onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled), 1)
print(onehot_reshaped)
print(type(onehot_reshaped))

# 희소행렬로 변환
onehot_fitted = onehot_encoder.fit_transform(onehot_reshaped)
print(onehot_fitted)
print(type(onehot_fitted))

# [2 2 2 2 0 0 0 0 0 0 2 2 0 1 1]
# <class 'numpy.ndarray'>
# [[2]
#  [2]
#  [2]
#  [2]
#  [0]
#  [0]
#  [0]
#  [0]
#  [0]
#  [0]
#  [2]
#  [2]
#  [0]
#  [1]
#  [1]]
# <class 'numpy.ndarray'>
#   (0, 2)	1.0
#   (1, 2)	1.0
#   (2, 2)	1.0
#   (3, 2)	1.0
#   (4, 0)	1.0
#   (5, 0)	1.0
#   (6, 0)	1.0
#   (7, 0)	1.0
#   (8, 0)	1.0
#   (9, 0)	1.0
#   (10, 2)	1.0
#   (11, 2)	1.0
#   (12, 0)	1.0
#   (13, 1)	1.0
#   (14, 1)	1.0
# <class 'scipy.sparse._csr.csr_matrix'>
```

## 피처 스케일링

각 변수에 들어 있는 수자 데이터의 상대적 크기 차이 때문에 머신러닝 분석 결과가 달라질 수 있다.

숫자 데이터들의 상대적인 크기 차이를 제거할 필요가 있다.

```python
import pandas as pd
import numpy as np

url = '/content/drive/MyDrive/Colab Notebooks/encore/csv/auto-mpg.csv'
df = pd.read_csv(url)
df.columns = ['mpg','cylinders','displacement','horsepower','weight',
              'acceleration','model year','origin','name'] 

# horsepower 열의 누락 데이터('?') 삭제하고 실수형으로 변환
df['horsepower'] = df['horsepower'].replace('?', np.nan)      # '?'을 np.nan으로 변경
df = df.dropna(subset=['horsepower'], axis=0)                 # 누락데이터 행을 삭제
df['horsepower'] = df['horsepower'].astype('float')      

# horsepower 열의 통계 요약정보로 최대값(max)을 확인
print(df['horsepower'].describe())

# count    391.000000
# mean     104.404092
# std       38.518732
# min       46.000000
# 25%       75.000000
# 50%       93.000000
# 75%      125.000000
# max      230.000000
# Name: horsepower, dtype: float64
```

## 데이터 전처리
    
### 정규화 : 데이터의 범위(크기) 일치
### labeling, one-hot 범주형 데이터를 수치화
### 수치형(연속형) : 카테고리화 pd_cut, np.histogram
### 결측치 처리(삭제, 대처) : ffill, bfill, billna
### 결측치가 존재하는 row를 출력
### info() : 타입과 결측치여부, 의심 : 숫자인데... object로 되어 있다.
### 중복 제거


----

## ERD
### 1. 데이터의 모양을 확인 head(), info(), type()
### 2. 결측치가 있는지 확인 isna().sum()
### 3. 결측치 처리 -> 삭제나 대체를 판단하여 적용
### 4. 범주형 데이터 -> 라벨링(labelencoder, one-hot: pd.get_dummies())
### 5. 수치형(연속형) - 구간 나누기 (ex 나이, 키)
### 6. 데이터의 크기 맞추기 - 정규화.. minmaxscaler
