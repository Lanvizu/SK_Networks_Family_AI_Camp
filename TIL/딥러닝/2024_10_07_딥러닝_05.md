## 순환 신경망 (RNN)

- 순서가 있는 시계열 데이터 처리에 적합
- 과거의 정보가 현재에 영향을 미치는 구조
- 과거의 정보를 기억해서 출력을 내는 구조

### 특징
- NLP, 음성 인식 등 연속적인 순서가 중요한 데이터 처리에 활용
- 은닉 상태가 이전 데이터의 출력값을 다음 데이터로 전달

### 문제점
- 입력 데이터가 길어질수록 초기 정보가 점점 사라지는 **장기 의존성 문제**
- **기울기 소실** 또는 **기울기 폭발** 문제 발생 가능

## RNN 변형 모델

### LSTM (Long Short-Term Memory)
- 셀 상태를 통해 정보를 오랜 시간 동안 유지하거나 삭제
- 입력 게이트, 망각 게이트, 출력 게이트로 구성

### GRU (Gated Recurrent Unit)
- LSTM의 간소화 버전
- 리셋 게이트와 업데이트 게이트만 사용
- LSTM보다 적은 매개변수로 비슷한 성능 달성

### 기타 변형 모델
- Bidirectional RNN: 양방향으로 정보를 처리
- Attention 메커니즘: 중요한 정보에 집중할 수 있는 구조

```python
from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/Colab Notebooks/encore/csv/주가예측.zip'

import pandas as pd

df = pd.read_csv('/content/train.csv')
df.head()
```

![image](https://github.com/user-attachments/assets/b040775b-4c71-4dc8-a645-79b512869972)

```python
# 탐색적 데이터 분석
# 데이터의 분포를 확인
import matplotlib.pyplot as plt
import seaborn as sns
# 개장가, 최고가, 최저가
data = df.loc[:, 'Open' : 'Low']
data['Close'] = df['Close']
data.hist()
```

![image](https://github.com/user-attachments/assets/a8671bef-316b-452c-97fb-db53d6b98555)

```python
# 데이터읽어서 정규화도  데이터셋
import numpy as np
from torch.utils.data import Dataset
class NDataSet(Dataset):
  # 초기화 데이터 로드
  def __init__(self):
    self.csv = pd.read_csv('/content/train.csv')
    # 정규화
    self.data = self.csv.loc[:,'Open':'Low'].values
    self.data = self.data / np.max(self.data) # 0과 1사이로 정규화
    # 정답 정규화
    self.label = self.csv['Close'].values
    self.label = self.label / np.max(self.label)  # __len__
  def __len__(self):
    return len(self.data) - 30 # 배치크기만큼 전체데이터에서 빼준다.
  # __getitem__  # 배치크기 정하기  30일 단위로
  def __getitem__(self,i):
    data = self.data[i:i+30]
    label = self.label[i+30]
    return data, label

dataset = NDataSet()
data.label = next(iter(dataset))
data
```
```python
class RNN(nn.Module):
  def __init__(self):
    super(RNN,self).__init__()
    # input_size 입력데이터크기(개장가,최고가,최저가)
    # hidden_size 은닉층의 개수
    # num_lyaers RNN을 5개 쌓는다
    # batch_first 입력데이이터를 (배치크기,시퀀스길이,입력크기)  (none,30,3)
    # 10 * 8 * 3 시퀀스*은닉층*입력데이터 크기
    self.rnn = nn.RNN(input_size = 3,hidden_size=8,num_layers=5,batch_first=True)
    # 분류기  예측  FC, MLP층 정의
    self.fc1 = nn.Linear(in_features= 240, out_features=64)
    self.fc2 = nn.Linear(in_features=64 , out_features=1)
    self.relu = nn.ReLU()
  def forward(self,x, h0): # x는 입력데이터 h0 초기 은닉상태
    x, hn = self.rnn(x, h0) # 출력 x(모든 타입스템에 대한 출력)와 최종은닉상태 hn을 반환
    # MLP 층으로 입력 모양 변경
    x = torch.reshape(x, (x.shape[0],-1))
    # mlp 층
    x = self.fc1(x)
    x = self.relu(x)
    x = self.fc2(x)
    # 예측한 종가를 1차원 벡터로 표현
    x = torch.flatten(x)
    return x
```
```python
# 데이터 로더
from torch.utils.data import DataLoader
dataset = NDataSet()
# 시계열 데이터는 섞지 않는다.
loader = DataLoader(dataset,batch_size=32)

# 모델 정의
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = RNN().to(device)

# 최적화
from torch.optim.adam import Adam
optim = Adam(params=model.parameters(),lr=0.001)
```

```python
from tqdm import tqdm
# 학습루프 정의
for epoch in range(200):
  iterator = tqdm(loader)
  for data,label in iterator:
    # 기울기 초기화
    optim.zero_grad()
    # 초기 은닉상태  모든 값을 0으로 초기화
    #(레이어수, 데이터의개수,hidden size)
    h0 = torch.zeros(5,data.shape[0],8).to(device)
    # 예측
    pred = model(data.type(torch.FloatTensor).to(device), h0)
    # 손실함수 계산
    loss = nn.MSELoss()(pred,label.type(torch.FloatTensor).to(device))
    # 역전파
    loss.backward()
    # 최적화
    optim.step()
    iterator.set_description(f'epoch:{epoch} loss:{loss.item()}')
torch.save(model.state_dict(), './rnn.pth') # 모델 저장
```

```python
# 모델 성늘 평가
import matplotlib.pyplot as plt
loader = DataLoader(dataset,batch_size=1)
# 에측값 저장 리스트
preds = []
total_loss = 0
with torch.no_grad():
  # 모델의 가중치 불러오기
  model.load_state_dict(torch.load('/content/rnn.pth', map_location= device))
  for data, label in loader:
    h0 = torch.zeros(5,data.shape[0],8).to(device) # 레이어 수, 데이터 수 , 히든 사이즈 수
    pred = model(data.type(torch.FloatTensor).to(device), h0)
    # 예측값을 리스트에 추가
    preds.append(pred.item())
    # 손실값을 계산
    loss = nn.MSELoss()(pred,label.type(torch.FloatTensor).to(device))
    total_loss += loss/len(loader)
print(f"loss : {total_loss.item()}")
# loss : 0.004546132870018482
```

```python
# 시각화
plt.plot(preds, label = 'prediction')
plt.plot(dataset.label[30:], label = 'real')
plt.legend()
plt.show
```

![image](https://github.com/user-attachments/assets/e80ac4fe-dd49-47a9-8ca7-6b2e89f42921)

