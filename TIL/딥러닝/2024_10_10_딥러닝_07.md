# 텐서플로 vs 파이토치

```python
# 텐서플로 : 예측 모델
import tensorflow as tf
from sklearn.model_selection import train_test_split

(x,y),(x_test, y_test) = tf.keras.datasets.california_housing.load_data()
x_train, x_val, y_train, y_val = train_test_split(x,y,random_state = 42)
```
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
x_test = scaler.transform(x_test)
```

```python
# 모델 구성
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation = 'relu', input_shape = x_train.shape[1:]),
    tf.keras.layers.Dense(15, activation = 'relu'),
    tf.keras.layers.Dense(1)
])
model.compile(loss = 'mse', optimizer = 'adam')
hist = model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 50)
test_loss = model.evaluate(x_test, y_test)
print(test_loss)

from sklearn.metrics import r2_score
y_pred = model.predict(x_test)
print(r2_score(y_test, y_pred))
# 5287532544.0
# 0.6022720336914062
```

```python
# 머신러닝 RandomForest
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(x_train, y_train)
model.score(x_test, y_test)
y_pred = model.predict(x_test)
print(r2_score(y_test, y_pred))
# 0.8076133005338257
```

-----

# PyTorch 기본 구조 및 학습 프로세스

## 1. 텐서 구조의 자료구조
- GPU/CPU 설정 (CUDA 활용)

## 2. 주요 구성 요소

### 2.1 데이터 처리
- **데이터셋과 데이터로더**
  - 상속 클래스 활용
  - 필수 메서드: `__getitem__`, `__len__`
  - 배치 크기 설정

### 2.2 모델 구현
- 상속을 통한 클래스 구현
- 주요 메서드:
  - `__init__`: 레이어 정의
  - `forward`: 전방 계산 (정의된 레이어 결합)

## 3. 학습 루프

```python
for epoch in range(num_epochs):
    for batch in dataloader:
        # 1. 옵티마이저 초기화 (기울기 리셋)
        optimizer.zero_grad()
        
        # 2. 모델에 입력 데이터를 넣어 예측값 생성
        predictions = model(input_data)
        
        # 3. 손실 함수를 사용해 손실값 계산
        loss = loss_function(predictions, targets)
        
        # 4. 역전파를 통한 기울기 계산
        loss.backward()
        
        # 5. 옵티마이저를 통해 모델 파라미터 업데이트
        optimizer.step()
```

------
```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
class DatasetCalifornia(torch.utils.data.Dataset):
  def __init__(self, x,y):
    self.x = torch.tensor(x, dtype = torch.float32)
    self.y = torch.tensor(y, dtype = torch.float32).unsqueeze(-1)
  def __len__(self):
    return len(self.x)
  def __getitem__(self, idx):
    return self.x[idx], self.y[idx]

from torch.utils.data import DataLoader
train_dataset = DatasetCalifornia(x_train, y_train)
val_dataset = DatasetCalifornia(x_val, y_val)
test_dataset = DatasetCalifornia(x_test, y_test)

train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = True)
val_dataloader = DataLoader(val_dataset, batch_size = 32, shuffle = True)
test_dataloader = DataLoader(test_dataset, batch_size = 32, shuffle = True)

# 모델 생성
class CalifoniaModel(nn.Module):
  def __init__(self):
    super(CalifoniaModel, self).__init__()
    self.fc1 = nn.Linear(8,30)
    self.fc2 = nn.Linear(30,15)
    self.fc3 = nn.Linear(15,1)
  def forward(self, x):
    x = torch.relu(self.fc1(x))
    x = torch.relu(self.fc2(x))
    x = self.fc3(x)
    return x

model = CalifoniaModel()
# 옵티마이저
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)
# 손실함수
loss_fn = nn.MSELoss()
# 학습루프
from tqdm import tqdm # 한줄로 쭉 업데이트?
epochs = 20
for epoch in range(epochs):
  iterator = tqdm(train_dataloader)
  for x,y in iterator:
    optimizer.zero_grad() # 기울기 초기화
    y_pred = model(x) # 예측
    loss = loss_fn(y_pred, y) # 손실값(텐서)
    loss.backward() # 기울기 계산
    optimizer.step() # 기울기 업데이트
    iterator.set_description(f'epoch {epoch+1} loss : {loss.item()}')
# 모델을 저장
torch.save(model.state_dict(), 'california.pt')
# 모델 불러오기
model = CalifoniaModel()
model.load_state_dict(torch.load('california.pt'))
# 모델 평가
test_loss = 0.0
with torch.no_grad():
  for x,y in test_dataloader:
    y_pred = model(x)
    loss = loss_fn(y_pred, y)
    test_loss += loss.item()
print(test_loss/len(test_dataloader))

from sklearn.metrics import r2_score
y_pred = model(torch.tensor(x_test, dtype = torch.float32)).detach().numpy()
print(r2_score(y_test, y_pred))
# 0.30630916357040405
```

-----


| 단계 | TensorFlow | PyTorch |
|------|------------|---------|
| 데이터 로드 | `tf.data.Dataset.from_tensor_slices()` 또는 `tf.keras.utils.image_dataset_from_directory()` | `torch.utils.data.Dataset` 상속 및 구현 |
| 데이터 전처리 | `tf.data.Dataset.map()` 또는 `tf.keras.layers.Rescaling` | `torchvision.transforms` 또는 커스텀 변환 함수 |
| 데이터 로더 생성 | `tf.data.Dataset.batch()` | `torch.utils.data.DataLoader` |
| 모델 구성 | `tf.keras.Sequential()` 또는 Functional API | `nn.Module` 상속 및 `forward` 메서드 구현 |
| 모델 컴파일 | `model.compile(optimizer, loss, metrics)` | 불필요 (옵티마이저, 손실 함수 별도 정의) |
| 옵티마이저 설정 | `tf.keras.optimizers.Adam()` | `torch.optim.Adam()` |
| 손실 함수 정의 | `tf.keras.losses.CategoricalCrossentropy()` | `nn.CrossEntropyLoss()` |
| 학습 루프 | `model.fit(dataset, epochs=10)` | 사용자 정의 학습 루프 구현 |
| 평가 | `model.evaluate(test_dataset)` | 사용자 정의 평가 루프 구현 |
| 예측 | `model.predict(input_data)` | `model(input_data)` |
| GPU 사용 | 자동 (설정 필요 없음) | `model.to('cuda')`, `tensor.to('cuda')` |
| 모델 저장 | `model.save('model.h5')` | `torch.save(model.state_dict(), 'model.pth')` |
| 모델 로드 | `tf.keras.models.load_model('model.h5')` | `model.load_state_dict(torch.load('model.pth'))` |
| 그래디언트 계산 | 자동 (Eager mode) | `loss.backward()` |
| 파라미터 업데이트 | 자동 (옵티마이저 내부에서 처리) | `optimizer.step()` |
| 배치 정규화 | `tf.keras.layers.BatchNormalization()` | `nn.BatchNorm2d()` |
| 드롭아웃 | `tf.keras.layers.Dropout(0.5)` | `nn.Dropout(0.5)` |
| 전이 학습 | `tf.keras.applications` 사용 | `torchvision.models` 사용 |
| 모델 평가 모드 설정 | `model.evaluate()` 내부에서 자동 처리 | `model.eval()` 명시적 호출 필요 |
| 추론 시 그래디언트 비활성화 | 자동 (`model.predict()` 사용) | `with torch.no_grad():` 컨텍스트 매니저 사용 |
| 사용자 정의 평가 지표 | `tf.keras.metrics.Metric` 상속 | `torch.nn.Module` 상속 또는 함수로 구현 |
| 조기 종료 | `tf.keras.callbacks.EarlyStopping` | PyTorch Lightning 또는 사용자 정의 구현 |
| 학습률 스케줄링 | `tf.keras.optimizers.schedules` | `torch.optim.lr_scheduler` |
