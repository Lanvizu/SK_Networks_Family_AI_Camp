### 캘리포니아 주택 데이터를 활용

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

housing = tf.keras.datasets.california_housing.load_data()
X = housing[0][0]
y = housing[0][1]
print(X.shape)
print(y.shape)

# (16512, 8)
# (16512,)
```

```python
input_wide = tf.keras.layers.Input(shape=[5]) # 5 0~4
input_deep = tf.keras.layers.Input(shape=[6]) # 6 2~7

norm_wide_layer = tf.keras.layers.Normalization()
norm_deep_layer = tf.keras.layers.Normalization()

h1 = tf.keras.layers.Dense(30, activation="relu")
h2 = tf.keras.layers.Dense(30, activation="relu")

concat = tf.keras.layers.Concatenate()
output = tf.keras.layers.Dense(1) # 값을 예측하는 것이므로 1

# 결합
input_wide = norm_wide_layer(input_wide)
input_deep = norm_deep_layer(input_deep)
x = h1(input_deep)
x = h2(x)
x = concat([x, input_wide])
output = output(x)
# 모델 생성
model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=output)
model.summary()
```

![image](https://github.com/user-attachments/assets/e6654374-fa61-4466-b408-60aa32955ca7)

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss="mse", optimizer=optimizer, metrics=["mae", "RootMeanSquaredError"])
# 데이터 분할
from sklearn.model_selection import train_test_split
x_train_full, x_test, y_train_full, y_test = train_test_split(X, y,test_size = 0.2, random_state=42)
x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full,test_size = 0.2, random_state=42)

x_wide, y_wide = x_train[:,:5], y_train[:5] # 0 ~ 4
x_deep, y_deep = x_train[:,2:], y_train[2:] # 2 ~ 7

history = model.fit([x_wide, x_deep], y_train, epochs=50, validation_data=([x_valid[:,:5], x_valid[:,2:]], y_valid))
```

```python
plt.plot(history.history["loss"], label='train_loss')
plt.plot(history.history["val_loss"], label='val_loss')
plt.legend()
plt.show()
```

![image](https://github.com/user-attachments/assets/6d12dae5-533d-41c9-aa1f-ad0e824f92ca)

```python
# 평가
model.evaluate([x_test[:,:5], x_test[:,2:]], y_test) # 학습 시 순서와 동일해야함.
# [8092108288.0, 68013.9140625, 89956.1484375]

from sklearn.metrics import r2_score
y_pred = model.predict([x_test[:,:5], x_test[:,2:]])
r2_score(y_test, y_pred)

# 0.385204017162323
```

### 보조 출력을 추가

```python
input_wide = tf.keras.layers.Input(shape=[5]) # 5 0~4
input_deep = tf.keras.layers.Input(shape=[6]) # 6 2~7

norm_wide_layer = tf.keras.layers.Normalization()
norm_deep_layer = tf.keras.layers.Normalization()

h1 = tf.keras.layers.Dense(30, activation="relu")
h2 = tf.keras.layers.Dense(30, activation="relu")

concat = tf.keras.layers.Concatenate()
output = tf.keras.layers.Dense(1) # 값을 예측하는 것이므로 1

# 결합
input_wide = norm_wide_layer(input_wide)
input_deep = norm_deep_layer(input_deep)
x = h1(input_deep)
x = h2(x)
x = concat([x, input_wide])
out = output(x)

# 보조 출력
out_ex = output(x)

# 모델 생성
model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[out, out_ex])

model.compile(loss =('mse','mse'), loss_weights=(0.9,0.1), optimizer='adam', metrics = ['mae','RootMeanSquaredError'])
model.summary()
```

![image](https://github.com/user-attachments/assets/7f744b66-df23-46e3-aeaa-b2ffbc9de498)

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss="mse", optimizer=optimizer, metrics=["mae", "RootMeanSquaredError"])

# 데이터 분할
from sklearn.model_selection import train_test_split
x_train_full, x_test, y_train_full, y_test = train_test_split(X, y,test_size = 0.2, random_state=42)
x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full,test_size = 0.2, random_state=42)

x_wide, y_wide = x_train[:,:5], y_train[:5] # 0 ~ 4
x_deep, y_deep = x_train[:,2:], y_train[2:] # 2 ~ 7

history = model.fit([x_wide, x_deep], y_train, epochs=50, validation_data=([x_valid[:,:5], x_valid[:,2:]], y_valid))
```

```python
plt.plot(history.history["loss"], label='train_loss')
plt.plot(history.history["val_loss"], label='val_loss')
plt.legend()
plt.show()
```

![image](https://github.com/user-attachments/assets/1f629c60-c647-44b3-b9c5-e16198a72c33)

```python
# 평가
model.evaluate([x_test[:,:5], x_test[:,2:]], y_test) # 학습 시 순서와 동일해야함.
[5456779264.0, 54649.46484375]
```

## checkpoint & early stopping
```python
fmnist = tf.keras.datasets.fashion_mnist.load_data()
X = fmnist[0][0]
y = fmnist[0][1]
print(X.shape)
print(y.shape)

# 데이터 분할
from sklearn.model_selection import train_test_split
x_train_full, x_test, y_train_full, y_test = train_test_split(X, y,test_size = 0.2, random_state=42)
x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full,test_size = 0.2, random_state=42)

# (60000, 28, 28)
# (60000,)
```

```python
input = tf.keras.layers.Input(shape=X.shape[1:])
norm = tf.keras.layers.Normalization()
flatten = tf.keras.layers.Flatten()
h1 = tf.keras.layers.Dense(30, activation="relu")
h2 = tf.keras.layers.Dense(30, activation="relu")
out = tf.keras.layers.Dense(10, activation="softmax")

# 결합
input = norm(input)
x = flatten(input)
x = h1(x)
x = h2(x)
output = out(x)

# 모델 생성
model = tf.keras.Model(inputs=input, outputs=output)

# 컴파일
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# 콜백

checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("my_keras_model.keras", save_best_only=True)
early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # patience 10은 이전 10단계를 비교해서 정한다라는 뜻.

# 학습
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])

# 평가
model.evaluate(x_test, y_test)
```
```python
model.evaluate(x_test, y_test)
# [0.4729350507259369, 0.8456666469573975]
```

------

# 배치 정규화 (Batch Normalization)

## 장점

- **학습 속도 향상**
- **과적합 감소**
- **깊은 신경망 학습 안정화**
- **가중치 초기화에 덜 민감**

## 단점

- **계산 복잡성 증가**
- **작은 배치에서 성능 저하**
- **추론 시 추가 연산 필요**
- **일부 시계열 데이터에 부적합**


```python
# 층을 설계
input = tf.keras.layers.Input(shape=X.shape[1:])
norm = tf.keras.layers.Normalization()
flatten = tf.keras.layers.Flatten()

batch_norm = tf.keras.layers.BatchNormalization()

h1 = tf.keras.layers.Dense(30)
h2 = tf.keras.layers.Dense(30)
act = tf.keras.layers.Activation("relu")
out = tf.keras.layers.Dense(10, activation="softmax")

# 층 결합
input = norm(input)
x = flatten(input)
x = h1(x)
x = batch_norm(x)
x = act(x)
x = h2(x)
x = batch_norm(x)
x = act(x)
output = out(x)

# 모델 생성
model = tf.keras.Model(inputs=input, outputs=output)
model.summary()
```

![image](https://github.com/user-attachments/assets/2c0a0ede-0a45-4f58-a16d-126200b4d0ec)

```python
# 콜백
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("my_keras_model.keras", save_best_only=True)
early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)

# 컴파일
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# 학습 
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])
```

```python
model.evaluate(x_test, y_test)
# [5.405930519104004, 0.1014999970793724]
```

## 전이 학습 (Transfer Learning)
전이 학습은 사전 학습된 모델을 새로운 작업에 활용하는 기법

## 학습률 스케줄링

학습률 스케줄링은 에포크별로 학습률을 동적으로 조정하는 기법

### 지수 감소 (Exponential Decay)

- 학습률을 지수적으로 감소시킴
- 수식: `lr = initial_lr * decay_rate ^ (step / decay_steps)`

### 구간별 감쇠 (Step Decay)

- 특정 구간마다 학습률을 크게 변경
- 예: 10 에포크마다 학습률을 절반으로 감소

### 다항 감쇠 (Polynomial Decay)

- 다항 함수를 사용하여 학습률 조정
- 수식: `lr = (1 - epoch / max_epochs) ^ power * initial_lr`

### 코사인 감쇠 (Cosine Decay)

- 코사인 함수를 사용하여 학습률 조정
- 수식: `lr = 0.5 * (1 + cos(pi * step / decay_steps)) * initial_lr`

### 사용자 지정 (Custom)

- 사용자가 직접 함수를 구현하여 적용
- Keras의 `LearningRateSchedule` 클래스를 상속하여 구현 가능

학습률 스케줄링은 모델의 성능 향상과 수렴 속도 개선에 도움

-----

```python
# CIFAR10
# 신경망
# 콜벡,배치노말,스케줄 등등 가능한 모든 자원을 사용해서 신경망 구성하고 학습
# 전이학습

from tensorflow.keras.applications import ResNet50
base_model = ResNet50()
base_model.summary()
```

```python
import tensorflow as tf
data = tf.keras.datasets.cifar10.load_data()
X = data[0][0]
y = data[0][1]
X.shape, y.shape
# ((50000, 32, 32, 3), (50000, 1))
```

```python
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)

input = tf.keras.layers.Input(shape=x_train.shape[1:])
flattern = tf.keras.layers.Flatten()
norm = tf.keras.layers.Normalization()
h1 = tf.keras.layers.Dense(100,activation='relu')
h2 = tf.keras.layers.Dense(100,activation='relu')
output = tf.keras.layers.Dense(10,activation='softmax')
# 조립
x = norm(input)
x = flattern(x)
x = h1(x)
x = h2(x)
output = output(x)
model = tf.keras.Model(input,output)
model.summary()
```

![image](https://github.com/user-attachments/assets/28d59a93-91ab-4ba0-97f4-2e4e1dad5639)

```python
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train,y_train,epochs=10, batch_size = 64, validation_split=0.2)
model.evaluate(x_test,y_test)
# [2.3034732341766357, 0.093299999833107]
```
