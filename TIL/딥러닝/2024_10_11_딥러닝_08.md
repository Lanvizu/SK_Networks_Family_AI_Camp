- GAN
  - 생성자 (Generator)
  - 판별자 (Discrimator)
  - 둘이 서로 경쟁하면서 학습
  
```python
import tensorflow as tf
from sklearn.model_selection import train_test_split
(x,y),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()
x = x.astype('float32')/255.0
x_test = x_test.astype('float32')/255.0
x_train,x_val,y_train,y_val = train_test_split(x,y, stratify=y, test_size=0.2)
x_train.shape, x_val.shape, x_test.shape

# ((48000, 28, 28), (12000, 28, 28), (10000, 28, 28))
```

```python
# fashion mnist
import tensorflow as tf
coding_size = 30  # 생성자에 입력되는 랜덤한 크기의 벡터 , 생성자가 처음 학습할때의 시작점
Dense = tf.keras.layers.Dense  # 여러번 반복되므로...
# 생성자
generator = tf.keras.Sequential([
  Dense(100, activation='relu',kernel_initializer='he_normal'),  # he_normal 안정적인 학습
  Dense(150, activation='relu',kernel_initializer='he_normal'),
  Dense(28*28, activation='sigmoid'),
  tf.keras.layers.Reshape([28,28])
])
# 판별자
discriminator = tf.keras.Sequential([
    tf.keras.layers.Flatten(),
    Dense(150, activation='relu',kernel_initializer='he_normal'),
    Dense(100, activation='relu',kernel_initializer='he_normal'),
    Dense(1, activation='sigmoid')
])
gan = tf.keras.Sequential([generator, discriminator])  # 두 네트웍을 결합
# GAN Adam 이나 rmsprop
discriminator.compile(loss='binary_crossentropy', optimizer='rmsprop') # 진짜인지 가짜인지 구분하는 판별자
discriminator.trainable = False  # 가중치를 고정하는 역활  GAN에서 생성자만 학습되도록
gan.compile(loss='binary_crossentropy', optimizer='rmsprop')

# 배치크기  - 너무 작으면 빠르고 학습이 불안정 , 너무크면 학습이 느려질수 있지만 안정적인 학습
batch_size = 32  # 64 128
# 데이터셋
dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000)  # 데이터를 하나씩 넘겨줌   1000 버퍼사이트의 크기
# 일부신경망 고정된 배치크기를 필요로 할때 유용한 방법
dataset = dataset.batch(batch_size,drop_remainder=True)  # 배치크기를 유지하기위해서 남는 데이터는 사용 안한다
dataset.prefetch(1) # 성능최적화를 위한 설정 모델 현재배치를 처리하는동안 다음배치를 미리 준비해서 처리속도 향상
```

```python
def plot_multiple_images(images, n_cols=None):
    n_cols = n_cols or len(images)
    n_rows = (len(images) - 1) // n_cols + 1
    if images.shape[-1] == 1:
        images = images.squeeze(axis=-1)
    plt.figure(figsize=(n_cols, n_rows))
    for index, image in enumerate(images):
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(image, cmap="binary")
        plt.axis("off")
```

```python
# 훈련 함수
def train_gan(gan,dataset,batch_size,codding_size, n_epochs):
  generator, discriminator = gan.layers
  for epoch in range(n_epochs):
    print(f'에포크 : {epoch+1}')
    for x_batch in dataset:
      # 판별자 훈련 - 랜덤벡터데이터 - 노이즈데이터
      noise = tf.random.normal(shape=[batch_size,codding_size])
      generated_images = generator(noise)  # 가짜이미지 생성
      # 생성된 가짜 이미지와 실제 이미지를 하나로 합침
      x_fake_and_real = tf.concat([generated_images,x_batch],axis=0)
      # 정답 레이블
      y1 = tf.constant([[0.]]*batch_size + [[1.]]*batch_size)
      # 판별자 훈련
      discriminator.train_on_batch(x_fake_and_real,y1)
      # 생성자 훈련
      noise = tf.random.normal(shape=[batch_size,codding_size])
      y2 = tf.constant([[1.]]*batch_size)  # 생성자는 판별자를 속이는 목적이므로 판별자에게 1이라고 인시되도록 생성
      gan.train_on_batch(noise,y2)  # gan 전체를 학습 - 판별자는 고정, 생성자만 학습
    # 훈련과정에서 생성자 얼마나 진짜 같은 이미지를 생성하는지 시각화
    plot_multiple_images(generated_images.numpy(), 8)
    plt.show()

# 학습
train_gan(gan,dataset,batch_size, coding_size, n_epochs=10)
```

-----

```python
# GNN - CNN 추가

# 데이터 로드
import tensorflow as tf
from sklearn.model_selection import train_test_split
(x,y),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()
x = x.astype('float32')/255.0
x_test = x_test.astype('float32')/255.0
x_train,x_val,y_train,y_val = train_test_split(x,y, stratify=y, test_size=0.2)
x_train.shape, x_val.shape, x_test.shape
# ((48000, 28, 28), (12000, 28, 28), (10000, 28, 28))
```

```python
# 생성자
import tensorflow as tf
coding_size = 100
generator = tf.keras.Sequential([
    tf.keras.layers.Dense(7*7*128, input_shape=[coding_size]),
    tf.keras.layers.Reshape([7,7,128]),  # 공간을 작게 만들어서 많은 채널을 가지는 특징맵
    tf.keras.layers.BatchNormalization(),
    # 업셈플링 - 더 큰 크기의 이미지를 생성  strides=2 2배로키움  14 14 64
    tf.keras.layers.Conv2DTranspose(64,kernel_size=5,strides=2,padding='same',activation='relu'),
    tf.keras.layers.BatchNormalization(),
    # fashion mnist channel 1 흑백 28 28 1
    # tanh 출력값을 -1 ~ 1 사이로 변경 GAN 이미지 픽셀을 -1 ~ 1 사이로  정규화 해서 사용할 예정
    tf.keras.layers.Conv2DTranspose(1,kernel_size=5,strides=2,padding='same',activation='tanh'),
])
descriminator = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64,kernel_size=5,strides=2,padding='same',
                           activation=tf.keras.layers.LeakyReLU(0.2)),  # 입력이 음수이면 작은 값 0.2곱해서 출력
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Conv2D(128,kernel_size=5,strides=2,padding='same',
                           activation=tf.keras.layers.LeakyReLU(0.2)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1,activation='sigmoid')
])
gan = tf.keras.Sequential([generator,descriminator])
```

```python
descriminator.compile(loss='binary_crossentropy', optimizer='adam')
descriminator.trainable = False
gan.compile(loss='binary_crossentropy', optimizer='adam')
```

```python
# 스케일링 및 크기변경
# x_train 0 ~ 1  -1 ~ 1
x_train_scaled = x_train.reshape(-1,28,28,1)*2-1  # -1 ~ 1
```
```python
# 학습
import matplotlib.pyplot as plt
batch_size = 16
dataset = tf.data.Dataset.from_tensor_slices(x_train_scaled).shuffle(1000)
dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)
train_gan(gan,dataset,batch_size,coding_size,n_epochs=10)
```

-----

 - 강화학습
   - 자전거 타기
   - 스키너의 행동심리학
   - 바둑 ,비디오 게임

- 학습사이클
  - 행동(에이전트) ->상태변화 ->보상

- 다중 손잡이 밴딧 문제
- OpenAI  gym라이브러리

- 에이전트/환경 : 특정환경에서 행동을 취함, 행동에 대한 결과(보상)
- 상태 : 현재상태, 에이전트가 이 상태를 바탕으로 행동을
결정
- 행동 : 에이전트가 취할수 있는 선택
- 보상 : 행동에 대한 피드백
- 정책 : 에이전트가 특정상태에서 어떤 행동알 선택할지 전략
- 가치 함수 : 보상의 총합


 - 다중손잡이 밴딧
  - 1을 넣고 여러손잡이중에서 하나를  골라서 당기면 1잃거나 얻음
  - 손잡이마다가 승률이정해져 있음, 사용자는 모름
  - 행동의 집합(손잡이1,손잡이2,손잡이3,손잡이4,손잡이5)
  - 보상의 집합 - {-1. 1}
  - 행동 -> 상태 변화 -> 보상의 학습 사이클에서 상태가 없는 단순한 문제

```
# 탐험형 탐색
  # 처음부터 끝가지 무작위로 선택하는 극단적인 탐험형 탐색
# 탐사형 정책
  # 몇번 시도해 보고 이후에는 그때까의 승률이 가장 높은 손잡이만 당기는
# 균형이 중요함 : 현재까지 높은 확률을 보이는 손잡이를 더 자주 당기지만 일정한 비율로 다른 손잡이도 시도하는 정책
# 에피소드 : 강화학습에서 게임시작~마칠때 까지

```

```python
import numpy as np
arms_profit = [0.4,0.12,0.52,0.6,0.25]  # 승률
n_arms = len(arms_profit)
# 손잡이를 당기는 횟수(에피소드의 길이)
n_trai = 1000
# 손잡이 당기는 시뮬레이션 함수
def pull_bandit(handle):
  q = np.random.random()  # 0~ 1사이의 난수
  if q < arms_profit[handle]:
    return 1
  else:
    return -1
# 랜덤 탐색정책 : 무작위로 손잡이를 당기고 보상을 기록하는 함수
def random_exploration():
  episode = []
  num = np.zeros(n_arms)
  wins = np.zeros(n_arms)
  for i in range(n_trai):
    h = np.random.randint(0,n_arms)
    reward = pull_bandit(h)
    episode.append([h,reward])
    num[h] += 1
    wins[h] += reward
  return episode,(num,wins)

e,r = random_exploration()
r
result = [ r[1][i]/r[0][i] if r[0][i] > 0 else 0.0  for i in range(n_arms) ]
print(f"손잡이별 승리 확률 : {result}")
wins = [ 2*r[1][i] - r[0][i] for i in range(n_arms)]  # 2*승리횟수 - 당긴횟수
print(f"손잡이별 수익 : {wins}")

# 손잡이별 승리 확률 : [-0.164021164021164, -0.7971014492753623, 0.14423076923076922, 0.14953271028037382, -0.4945054945054945]
# 손잡이별 수익 : [-251.0, -537.0, -148.0, -150.0, -362.0]

```

```python
!pip install gym
!pip install torch torchvision torchaudio

import gym
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import matplotlib.pyplot as plt

# DQN 네트워크 정의
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 24)  # state_size 관측값의 차원
        self.fc2 = nn.Linear(24, 24)
        self.fc3 = nn.Linear(24, action_size)  # 가능한 행동의 수

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# DQN 에이전트 클래스 정의
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.99  # 할인 계수
        self.epsilon = 1.0  # 탐사 확률
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.criterion = nn.MSELoss()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        state = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.model(state)
        return np.argmax(q_values.detach().numpy())

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in batch:
            target = reward
            if not done:
                target += self.gamma * np.amax(self.model(torch.FloatTensor(next_state)).detach().numpy())
            target_f = self.model(torch.FloatTensor(state))
            target_f = target_f.clone()  # Clone the target_f tensor to avoid in-place modification
            target_f[action] = target  # 1차원 텐서로 처리
            self.optimizer.zero_grad()
            loss = self.criterion(self.model(torch.FloatTensor(state)), target_f)
            loss.backward()
            self.optimizer.step()

# Hyperparameters
EPISODES = 1000
BATCH_SIZE = 32

# 환경 초기화
env = gym.make("CartPole-v1", new_step_api=True)  # 새로운 API 사용
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = DQNAgent(state_size, action_size)

rewards = []

# DQN 학습
for e in range(EPISODES):
    state = env.reset()  # 상태 초기화
    total_reward = 0
    done = False
    while not done:
        action = agent.act(state)
        next_state, reward, done, truncated, _ = env.step(action)  # 새로운 API에 맞게 수정
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

    agent.replay(BATCH_SIZE)
    rewards.append(total_reward)

    if agent.epsilon > agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

    if e % 100 == 0:
        print(f"Episode: {e}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}")

# 보상 시각화
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Rewards over Episodes')
plt.show()

env.close()

```
